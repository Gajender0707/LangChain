{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c518b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac77a1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sanju/Documents/DS/Generative AI/Langchain'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909bc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c2a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1082fc3b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"data/speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a7623d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/speech.txt'}, page_content='1. Amazon’s AI-Driven Customer Support\\n\\nAI Chatbots and Virtual Assistants\\n\\nAmazon employs AI chatbots and virtual assistants to streamline customer interactions. These tools handle routine inquiries, allowing human agents to focus on complex issues. Amazon Lex, a service for building conversational interfaces, powers these chatbots, enabling natural language understanding and speech recognition .\\n\\nAlexa+: The Next-Generation Assistant\\n\\nIn 2025, Amazon introduced Alexa+, an AI-enhanced version of its voice assistant. Alexa+ offers more natural conversations, personalized experiences, and the ability to complete tasks like booking rides or retrieving videos from Ring cameras . This upgrade utilizes advanced language models to optimize performance across various tasks')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf77644",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the pdf using this \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader=PyPDFLoader(\"data/task2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9de5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./myenv/lib/python3.12/site-packages (5.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bs4 in ./myenv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./myenv/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf\n",
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d402da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='How AI is Changing the Face of \\nCustomer Service – A Case Study on \\nAmazon & Zappos \\nAbstract\\nWith the rise of digital technology, companies are under pressure to offer faster and more efﬁcient \\ncustomer support. Artiﬁcial Intelligence (AI) has stepped in to ﬁll this gap. In this case study, we \\nexplore how Amazon and Zappos are using AI tools like chatbots and virtual assistants to improve \\ncustomer experience. We’ll look at how machine learning models help resolve customer queries \\nquicker, what challenges exist in making AI feel more human, and what the future holds for AI in \\ncustomer support. This study also includes a mini-project proposal where students can build a basic \\nchatbot.\\n1. Introduction\\nCustomer service is a key factor in customer satisfaction and loyalty. With millions of daily users, \\ncompanies like Amazon and Zappos need solutions that scale and respond quickly. AI is helping \\nthem manage huge volumes of customer queries efﬁciently. While these tools can’t replace human \\nempathy, they’re getting better at handling common problems and saving time for both companies \\nand customers.\\n2. How Amazon Uses AI in Customer Service'),\n",
       " Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content=\"Amazon has built one of the most advanced AI systems in the customer service world. Here's how \\nthey use it:\\n2.1 Chatbots and Virtual Assistants\\nAmazon’s customer service chatbots can answer questions about orders, deliveries, returns, and \\nmore. These bots are built using Amazon’s own tools like Amazon Lex and are available 24/7.\\n2.2 Alexa for Customer Help\\nAmazon Alexa isn’t just for music or news. It can also tell you where your order is, help with \\nsimple troubleshooting, and connect you to a human agent if needed. This hands-free method is \\ngreat for busy users.\\n2.3 Machine Learning Models\\nBehind the scenes, Amazon uses data to learn what customers need. Their systems automatically \\nroute queries, prioritize important issues, and offer personalized product recommendations based on \\nbrowsing and purchase history.\\n3. How Zappos Uses AI Differently\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Zappos is known for its unique approach to customer service. Even though it’s owned by Amazon, \\nit does things a bit differently.\\n3.1 A Mix of Humans and AI\\nZappos believes in human connections. So, while it uses AI to ﬁlter and understand basic customer \\nneeds, it relies on human agents to solve more complex problems. This balance helps keep the \\npersonal touch.\\n3.2 Smarter Customer Data Tools\\nAI is used behind the scenes to help agents know the customer better. Based on previous purchases \\nand chats, Zappos’ systems give suggestions to customer reps, which leads to quicker and more \\npersonalized support.\\n3.3 Sentiment Detection\\nZappos uses AI to read the tone of messages. If a customer sounds angry or upset, the system \\nhighlights the case for faster human handling. This way, serious complaints get priority.\\n4. Comparing Traditional and AI-Powered Customer Support\\nDiagram: Chatbot Workﬂow [Visual idea: Customer > Bot understands intent > Picks response > \\nSends reply or forwards to human if needed]\\n5. Why AI Helps in Customer Service\\n• Shorter Wait Times: Bots can talk to hundreds of people at once.\\n• Faster Problem Solving: They quickly pull up order or product info.\\n• Lower Costs: One bot can do the job of many agents for simple tasks.\\n• Smart Routing: Urgent queries go to the right person or team automatically.\\nFeature Traditional Support AI-Powered \\nSupport\\nAvailable \\nHours Limited (usually 9–5) 24/7 Availability\\nHandling \\nSpeed\\nSlow for high \\nvolumes Fast and scalable\\nCost High labor cost Lower long-term cost\\nPersonal Touch High Depends on training\\nConsistency Varies by agent Always consistent'),\n",
       " Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='6. Sentiment Analysis & Auto-Response: How They Work\\n6.1 Understanding Emotions with Sentiment Analysis\\nAI tools try to read emotions from the way people type. For example:\\n• \"I need this ﬁxed right now!\" might be ﬂagged as urgent or negative.\\n• \"Thanks for the help!\" is seen as a positive comment.\\nThis helps companies decide when to get a human involved or when to offer a discount or apology.\\n6.2 Quick Replies with Auto-Response Tools\\nInstead of waiting for a human, these tools create replies using what they’ve learned from past \\nchats. For example:\\nCustomer: \"When will my order #789456 arrive?\" Bot: \"Your order was shipped on April 15 and \\nshould arrive by April 20.\"\\nThese tools understand what the user wants, ﬁnd the info, and send it—almost instantly.\\n7. The Future of AI in Customer Support'),\n",
       " Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='7.1 Smarter Personalization\\nIn the future, AI might remember how each user likes to be helped. Some customers want quick \\nanswers, others want detailed steps. AI will adjust accordingly.\\n7.2 More Human-Like Bots\\nVirtual assistants may soon recognize voice tones, facial expressions (on video), and even mood. \\nThis can help bots respond more naturally.\\n7.3 Helping Before You Ask\\nAI might one day predict what help you need before you reach out. For example, if you often return \\nshoes after buying a certain size, the system might suggest a better size before checkout.\\n8. Problems with AI Support\\n• Robotic Feel: Bots still can’t match human empathy.\\n• Misunderstanding Context: Jokes, sarcasm, or slang can confuse them.\\n• Bias in Data: If AI is trained on biased info, it may treat people unfairly.\\n• Privacy Issues: Customers may worry about how their data is used.\\n• Ethical Concerns: Should bots pretend to be human? How much should they know?\\n9. Mini-Project: Build a Simple Chatbot\\nGoal:\\nMake a chatbot that can answer basic customer service questions.\\nTools Needed:\\n• Python\\n• Flask (to run it on a website)\\n• NLTK or spaCy (to understand language)\\n• SQLite (for storing common questions and answers)\\nWhat It Should Do:\\n• Answer questions like \"What is your return policy?\"\\n• Understand the mood of the user'),\n",
       " Document(metadata={'producer': 'macOS Version 15.4 (Build 24E248) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250417073851Z00'00'\", 'title': 'task2', 'moddate': \"D:20250417073851Z00'00'\", 'source': 'data/task2.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content=\"• Ask for feedback after the chat ends\\nExtra Features (Optional):\\n• Let users talk using voice\\n• Show the bot in a web app\\n10. Final Thoughts\\nAmazon and Zappos use AI in different ways to make customer support better. Amazon focuses \\nmore on speed and automation. Zappos uses AI but still values human connection. Both show how \\nAI can make support faster and smarter. As the technology grows, AI could even predict problems \\nbefore they happen. But it's also important to remember the human side—empathy, understanding, \\nand trust are still key in customer service.\\nReferences\\n• Amazon Developer Docs\\n• Articles from Forbes, HBR\\n• Zappos customer support interviews\\n• Research on NLP, AI Ethics, and Sentiment Analysis\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=pdf_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be86e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in ./myenv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./myenv/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8adf3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## reading the webpage using the langchain_community.documents_loders\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "web_loader=WebBaseLoader(web_paths=(\"https://www.airwaysmag.com/\",),\n",
    "                         bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                             class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                         )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093a789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.airwaysmag.com/'}, page_content='')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e58958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227f8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query related to your NLP topic\n",
    "query = \"large language models for question answering\"\n",
    "\n",
    "# Load documents (you can set how many papers to load)\n",
    "loader = ArxivLoader(query=query, load_max_docs=3)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945e8613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-11-29', 'Title': 'Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models', 'Authors': 'Xinghang Hu', 'Summary': 'Community Question Answering (CQA) becomes increasingly prevalent in recent\\nyears. However, there are a large number of answers, which is difficult for\\nusers to select the relevant answers. Therefore, answer selection is a very\\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\\ncross attention networks (QAN) with pre-trained models for answer selection and\\nutilize large language model (LLM) to perform answer selection with knowledge\\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\\npre-training for question subjects, question bodies and answers, respectively,\\nthen the cross attention mechanism selects the most relevant answer for\\ndifferent questions. Experiments show that the QAN model achieves\\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\\nMoreover, we use the LLM to generate external knowledge from questions and\\ncorrect answers to achieve knowledge augmentation for the answer selection task\\nby LLM, while optimizing the prompt of LLM in different aspects. The results\\nshow that the introduction of external knowledge can improve the correct answer\\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\\ncan also select the correct answer on more questions by optimized prompt.'}, page_content='Enhancing Answer Selection in Community Question\\nAnswering with Pre-trained and Large Language Models\\nXinghang Hu1\\naPricewaterhouseCoopers, China\\nAbstract\\nCommunity Question Answering (CQA) becomes increasingly prevalent in\\nrecent years. However, there are a large number of answers, which is difficult\\nfor users to select the relevant answers. Therefore, answer selection is a very\\nsignificant subtask of CQA. In this paper, we first propose the Question-\\nAnswer cross attention networks (QAN) with pre-trained models for answer\\nselection and utilize large language model (LLM) to perform answer selection\\nwith knowledge augmentation. Specifically, we apply the BERT model as the\\nencoder layer to do pre-training for question subjects, question bodies and\\nanswers, respectively, then the cross attention mechanism selects the most\\nrelevant answer for different questions.\\nExperiments show that the QAN\\nmodel achieves state-of-the-art performance on two datasets, SemEval2015\\nand SemEval2017. Moreover, we use the LLM to generate external knowl-\\nedge from questions and correct answers to achieve knowledge augmenta-\\ntion for the answer selection task by LLM, while optimizing the prompt of\\nLLM in different aspects. The results show that the introduction of external\\nknowledge can improve the correct answer selection rate of LLM on datasets\\nSemEval2015 and SemEval2017. Meanwhile, LLM can also select the correct\\nanswer on more questions by optimized prompt.\\nKeywords:\\nCross attention, Large language model, Knowledge\\naugmentation, Prompt optimization\\n1. Introduction\\nCommunity Question Answering (CQA), as shown in Figure 1, has gained\\npopularity in recent years, offering an interactive experience and faster infor-\\nmation retrieval across various domains. For instance, according to the latest\\nPreprint submitted to Elsevier\\nNovember 30, 2023\\narXiv:2311.17502v1  [cs.CL]  29 Nov 2023\\nQuestion\\nAny suggestion for Spa having the facility of \\nOil Baths？\\nAnswer\\nAnswer1: It could be a good defence against \\nintruders if any… \\nAnswer2: There is Turkish Hamams, I will \\nhave to check with my QL buddy \\nKellyheroes and get back to you!\\nAnswer Ranking and Selection Methods \\nResult: Answer2\\nFigure 1: A schematic of a community question answering system\\navailable data, Quora boasts approximately 190 million active monthly users.\\nAdditionally, the total number of users visiting Quora, including unregistered\\nusers, reaches approximately 633 million per month, with 300 million being\\nunique visitors. CQA platforms like Yahoo! Answers, StackOverflow, and\\nQuora provide users with an open environment to search for information\\nof interest, post their own inquiries, and contribute answers based on their\\nknowledge.\\nHowever, due to the diverse backgrounds of the participants, the qual-\\nity of answers can vary significantly. Some responses align with the topic\\nand meet the users’ expectations, while others deviate completely from the\\nintended purpose. Consequently, users face the challenge of sifting through\\nnumerous candidate answers to find the most relevant one, which can be time-\\nconsuming. Furthermore, the number of repeated and unanswered questions\\nhas significantly increased. Many of these questions have, in fact, been pre-\\nviously asked and answered by different users but with slight variations in\\nwording or syntax. Therefore, answer selection becomes a crucial task in\\nCQA.\\nThe objective of answer selection is to identify the most pertinent answer\\nfrom the repository, effectively reducing the time users spend reviewing all\\ncandidate responses. This not only enhances the user experience but also\\nenables the resolution of similar, unresolved questions that have been an-\\nswered before but presented differently. By improving answer selection in\\n2\\nCQA, users can enjoy a more efficient and satisfying experience while bene-\\nfiting from the accumulated knowledge within the community. But the task\\nof answer selection poses significant challenges for two main reasons. Firstly,\\nit is complicated and tricky due to the presence of low-information words,\\nauxiliary verbs, a large number of synonyms, and syntactic transformations.\\nThis complexity can be observed in examples such as the questions “What\\nare better ways to look for nice local restaurants?” and “How do you search\\nfor great restaurants along your route?” which essentially convey the same\\nmeaning of seeking good places to eat, but differ in their choice of words and\\nsyntactic structures. Secondly, many researchers tend to treat questions and\\nanswers equally, overlooking the redundancies and noise present in answers,\\nas pointed out by Zhang et al [1]. This disregard for the informative content\\nin answers often leads to significant deviations. Answer selection provides an\\neffective solution to address these challenges. It involves selecting valuable\\nanswers from a list of candidate responses based on the semantic matching\\nbetween the question and the answer. Traditional approaches to answer se-\\nlection typically rely on recurrent neural networks (RNN), Long Short-Term\\nMemory (LSTM) recurrent neural networks, Seq2seq models, and other re-\\nlated technologies [2, 3]. With the emergence of attention mechanisms, such\\nas the Transformer and BERT technologies, utilizing attention for answer\\nselection has become a prominent research focus in the field of CQA [4, 5, 6].\\nNevertheless, current research in this area often neglects the separate\\nprocessing of questions and answers, failing to adequately address the in-\\nteraction between them. To address these challenges, we present QAN, a\\nQuestion-Answer Cross Attention Network with Pre-trained models for An-\\nswer Selection in CQA. QAN leverages BERT, a pre-trained language model,\\nfor word encoding, taking into account the contextual information of each\\nword in question subjects, question bodies, and answers. We employ cross\\nattention between words in question subjects and words in answers, as well\\nas between words in question bodies and words in answers, to capture inter-\\nactive information between questions and answers. Finally, the QAN model\\ncombines attention on questions and attention on answers to compute the\\nmatching probability of question-answer pairs, enabling the identification of\\nthe most suitable answer for a given question.\\nLarge Language Models(LLM) are deep learning models with massive\\nparameters used for natural language processing tasks, capable of under-\\nstanding context, generating text, and finding extensive applications across\\nvarious domains. In the field of question answering, users can ask questions\\n3\\nto an intelligent question answering system, and the LLM can help the sys-\\ntem understand the question and provide relevant suggestions such as in\\nmedicine [7, 8, 9] and materials science [10]. In addition, LLM can also be\\nused as an external knowledge engine for question answering systems [11].\\nAlthough LLM has been studied for Q&A tasks, few have been used for CQA,\\nlet alone for answer selection based on augmentation of external knowledge.\\nMeanwhile, Prompt has an important effect on the output of large language\\nmodels, but the related research is also less. To address these challenges, we\\nuse the external knowledge generated by the large language model LLaMa\\nto enhance the performance of the answer selection task performed by LLM,\\nand optimize the prompt input to LLM in different aspects to help select the\\ncorrect answer from more questions.\\nThe key contributions of our work are as follows:\\n• We utilize the BERT model for pre-training question subjects, ques-\\ntion bodies, and answers separately, capturing comprehensive seman-\\ntic information from both questions and answers.\\nAdditionally, we\\nemploy the cross attention mechanism to effectively capture impor-\\ntant interactive features between questions and answers. Our proposed\\nmodel achieves state-of-the-art performance on the SemEval2015 and\\nSemEval2017 datasets, outperforming existing answer selection models.\\n• We incorporate llama-7b-hf, a large language model, to generate knowl-\\nedge as the answer reference for questions of the datasets. This refer-\\nence enhances the alignment between questions and answers and im-\\nproves the model’s performance on the answer selection task.\\n• We optimized prompt from four different perspectives, enabling LLM\\nto select the right answers on more questions of the datasets, providing\\nideas and direction for prompt optimization.\\n2. Related Work\\nTraditional methods for answer selection can be categorized into con-\\ntent/user modeling methods and adaptive support methods. Content/user\\nmodeling methods focus on modeling user characteristics, questions, and\\ncorresponding answers to extract high-level attributes from low-level Q&A\\ninteractions, which are essential inputs for CQA functions. For instance, an-\\nswer quality evaluation outputs can be utilized to rank answers. Shah [12] et\\n4\\nal. constructed an improved evaluation system by extracting various features\\nfrom questions, answers, and the users who posted them. Adaptive support\\nmethods, based on content/user modeling, enhance user collaboration success\\nand effectiveness through question retrieval and question routing. Question\\nretrieval recommends archived question-answer pairs. Zhang et al. [13] pro-\\nposed a supervised question answering topic modeling method that matches\\nquestions not only at the term level but also at the topic level, demonstrat-\\ning strong retrieval performance in CQA. Question routing recommends the\\nbest potential answer, taking into account user expertise, user activity, and\\nmotivation. Zhao [14] approached the routing problem from the perspective\\nof missing value estimation and used graph regularization matrix completion\\nto estimate the missing values.\\nAttention mechanisms [15] have been widely adopted in question answer-\\ning tasks. Guo et al. [16] proposed a reattention framework for visual ques-\\ntion answering tasks. Zheng et al. [17] incorporated attention mechanisms\\nand bilinear technology to enhance features and address remote sensing vi-\\nsual question and answer tasks. Zhang et al. [4] designed a graph-based\\nthree-attention network to construct target-aware responder representations,\\nanswer-specific question representations, and context-aware answer represen-\\ntations through attentional computation. Ha et al. [6] integrated supervised\\nattention into match-LSTM, guiding attention weight learning for question-\\nanswer pairs with external lexical semantics, resulting in better performance\\ncompared to the baseline model. Cross attention within the attention mech-\\nanism is particularly adept at capturing interactive features between pro-\\ncessing questions and answers due to its ability to handle cross-sequence\\nrelationships.\\nLarge language models (LLM), exemplified by chatGPT, have gained sig-\\nnificant attention in natural language processing.\\nThere have been some\\nresearches on large language models for Q&A tasks. Guo et al. [18] focused\\non building a retrieval-based medical question answering system using large\\nlanguage models, semantic matching, named entity recognition, and knowl-\\nedge graphs to improve answer selection, achieving consistent improvements\\nover strong baselines on various datasets. Singhal et al. [7] presented Multi-\\nMedQA, a comprehensive benchmark for medical question answering, eval-\\nuates large language models, including Flan-PaLM with instruction prompt\\ntuning, achieving state-of-the-art accuracy but still existing gaps compared\\nto clinicians. Yu et al. [19] used large language model generators that replace\\ndocument retrievers to perform open-domain question answering, achieving\\n5\\nsignificant performance improvements without external document retrieval.\\nLee et al. [20] introduced automated feedback, and implemented a feedback\\nlearning loop to improve citation, correctness, and fluency in LLM-generated\\nresponses for QA systems.\\nWeng et al.\\n[21] introduced the Holistically\\nThought (HoT) method for enhancing large language models in addressing\\ncomplex medical conversational question answering tasks, and a better re-\\nsponse effect has been achieved.\\nIt can be seen that LLM have a broad\\napplication prospect in Q&A systems, but there are still few studies on CQA\\ntasks.\\nIn recent years, the research of knowledge augmentation for question an-\\nswering system has become a hot topic. Cai et al. [22] used the semantics\\nof Wikipedia to enrich questions and solve the task of large-scale question\\nclassification. Jing et al. [23] proposed a knowledge-enhanced attentionan-\\nswer selection (KAAS) model to improve the performance of CQA systems.\\nHuang et al. [24] proposed an interactive knowledge-augmented attention\\nnetwork for answer selection (IKAAS), which uses the external knowledge in\\nthe knowledge graph as learning information for question answering pairs,\\nand achieves good results. Li et al. [25] developed a well-performing answer\\nranking model by extending questions with wiki commonsense knowledge\\nand capturing relevant knowledge.\\nNguyen et al.\\n[26] combined features\\nfrom convolutional neural networks and other methods to obtain additional\\nknowledge to enhance deep learning models and achieved better results in\\nCQA tasks. It can be seen that knowledge enhancement is helpful to improve\\nthe correct rate of answer selection, so we also combine external knowledge\\nand questions and answers to perform CQA tasks.\\n3. Methodology for PLM\\nIn this section, we adopt PLM to make answer selection. We introduce\\nour proposed QAN model, including the mathematical expression and the\\ncomponents of the model.\\n3.1. Task Description\\nIn this research, the answer selection task in CQA can be described as a\\ntuple of four elements (S, B, A, y). S = [s1, s2, · · · , sm] represents a question\\nsubject whose length is m. B = [b1, b2, · · · , bg] represents the corresponding\\nquestion body whose length is g. A = [a1, a2, · · · , an] represents the corre-\\nsponding answer whose length is n. And y ∈Y represents the relevance\\n6\\nCross Attention\\nQsubject_Answer\\nQbody_Answer\\nResult\\nQuestion Subject\\nAnswer\\nQuestion Body\\nAnswer\\nBERT\\nBERT\\nInteraction and Prediction\\nFigure 2: Pipeline of answer selection with PLMs.\\ndegree, namely, y = {Good, Potential, Bad} to determine whether a candi-\\ndate can answer a question properly or not. More detailed, Good represents\\nthat the answer can provide a proper solution for the question, while Poten-\\ntial indicates that the answer might provide a useful solution to users and\\nBad means the answer is not relevant to the question. Generally, our QAN\\nmodel on the answer selection task in CQA can be summarized as assigning\\na label to each answer based on the conditional probability Pr(y | S, B, A)\\nwith the given set {S, B, A}.\\n3.2. Overview of Proposed Model\\nOur proposed QAN model consists of three key layers and the framework\\nis shown in Figure 2. We first utilize the BERT [27] to capture contextual rep-\\nresentations of the question subject, question body, and answer in token form.\\nNext, we use cross attention mechanism [28] to establish essential interaction\\nfeatures between the question and answer by analyzing the relationships be-\\ntween the question subject-answer and question body-answer. Specifically,\\nthis Cross Attention Layer takes encoded form ouputs by BERT encoder as\\ninput and compute the relevance between each word in question subjects\\nand answers as well as the counterpart in question bodies and answers. Take\\nthe process of computing relevance between each word in question subjects\\n7\\nand answers as an example, we get a matrix. Then two similarity matrices,\\nquestion subjects to answers and answers to question subjects, are generated\\nafter normalization over each row and column by softmax function of the\\nsimilarity matrix. In similarity matrices, rows represent question subjects,\\nand columns represent answers.\\nAnd we get interactive features between\\nquestion subjects and answers as the final output. After that, we propose\\nthe Interaction and Prediction Layer, inspired by previous work [29, 30], pro-\\ncesses interaction features and leverages acquired knowledge to assign labels\\nto each answer in response to the given question, using conditional proba-\\nbility calculations. Specifically, we train our model with bidirectional GRU\\n(Bi-GRU) to acquire context information between questions and their cor-\\nresponding answers. After that, we use max pooling and mean pooling of\\nquesitons and answers to acquire fixed-length vectors. Then, we concatenate\\nthese vectors to get the global representation r. Finally, we pass the global\\nrepresentation r to the prediction layer which is consisted of a multi-layer\\nperceptron (MLP) classifier to determine whether the semantic meaning of\\nthe give question-answer pair is equivalent or not. These layers are hierar-\\nchically organized, with Figure 1 depicting the BERTCAN model’s workflow\\nfrom top to bottom.\\n4. Methodology for LLM\\nIn this section, we adopt LLM for answer selection. It includes knowl-\\nedge retrieval, knowledge augmentation and prompt optimization. We use\\nLlama-7b-hf to generate external knowledge and perform answer selection.\\nLlama-7b-hf is a collection of pretrained and fine-tuned generative text mod-\\nels ranging in scale of 7 billion parameters.\\n4.1. Task Description\\nThe BERTCAN model we proposed is used for answer multi-classification\\ntasks. However, for LLM, their multi-classification task capabilities are not\\nideal [31], so the task of LLM is changed to generating answer selection\\nquestions, that is, selecting the correct answers to the questions.\\nIn the\\nSemEval2015 and SemEval2017 data sets, the correct answer is the answer\\nwhere its “CGold” is “Good”. The whole process is shown in Figure 3. In\\nthis research, the answer selection task in CQA by LLM can be described as\\na tuple of six elements (S, B, A, K, P, y). S represents a question subject. B\\nrepresents the corresponding question body. A represents the corresponding\\n8\\nResult\\nLarge Language Model\\nKnowledge\\nLarge Language Model\\nPrompt\\nAnswer\\nQuestion\\nFigure 3: Pipeline of answer selection with LLMs.\\nanswers.\\nK represents the corresponding knowledge to the question.\\nP\\nrepresents the prompt that is input to LLM. And y represents the output of\\nthe LLM. Specifically, we combine question subject, question body, answer,\\nknowledge and prompt, and then input the combined content into the LLM.\\nLLM will output what it thinks is the most appropriate answer based on the\\ninput.\\n4.2. Prompt Optimization\\nPrompt has a crucial impact on the output of LLM. For the same ques-\\ntion, different prompts may lead to different answers, as shown in Table 1.\\nFrom the Table 1, we can see that not all prompts could select the right\\nanswer from the question that one prompt could not lead to the right op-\\ntion while another prompt could. Because Prompt2 is optimized compared\\nto Prompt1. Consequently, the optimization of prompts on LLM is a key\\napproach to improve the performance of answer select of CQA by LLM.\\nIn order to improve the accuracy of answer selection as much as possible,\\nPrompt needs to be optimized. We optimize Prompt in terms of prompt\\nlength, position of questions and answers, question subject, position of task\\ndescription.\\n4.3. Knowledge Retrieval\\nRelated studies have shown that knowledge augmentation, such as incor-\\nporating external knowledge graph, is helpful for improving the performance\\nof question answering tasks [23, 32, 33]. Considering that external knowl-\\nedge enhancement may be beneficial to improving the accuracy of answer\\nchoices, we combine the text of the question and the Good answer to the\\n9\\nTable 1: An example where different prompts result in different results\\nQuestion\\nOptions\\nRight\\nOpiton\\nPrompt1\\nPrompt2\\nSelection\\nof\\nPrompt1\\nSelection\\nof\\nPrompt2\\nRecently\\nIndian\\nGovernment\\nbought\\n200\\nTons\\nof\\nGold\\nfrom\\nother\\ncoun-\\ntry....The\\nGold\\nprice\\nsuddenly\\nincrease\\nto\\na\\ntremendously\\nhigh.\\nWhy\\nIndian\\nso\\ncrazy\\nin\\nwearing\\nGold or golden acces-\\nsories...their men and\\nwomen\\nwearing\\ngold\\nor\\ngolden\\njewelry;\\naccessories\\nextraordi-\\nnarily...friend of mine\\nwearing\\ngolden\\nrings\\non 3 of his finger right\\nand left...it’s insane...\\nC1:It’s a safe\\ninvestment\\nand\\nthey\\nhave\\nbeen\\ndoing it for a\\nlong time.\\nC2:And\\nyour concern\\nabout\\nit\\nis?\\nOne\\nlife\\nto\\nlive; live it to\\nthe fullest.\\nC1\\nUtilizing the\\ninformation\\nin\\nhint-\\n[KNOWLEDGE],\\nchoose\\nthe\\nserial\\nnum-\\nber\\nof\\nthe\\noptimal\\nre-\\nsponse\\nto\\nthe question-\\n[QUESTION]\\nthat\\nis\\ndis-\\ntinct\\nfrom\\nthe\\noptions-\\n[ANSWER].\\nGiven\\nthe\\ndiscov-\\neries\\nfrom\\nhint-\\n[KNOWLEDGE],\\nyour task is to discern\\nand specify the unique\\nserial number tied to\\nthe most appropriate\\nanswer\\nto\\nquestion-\\n[QUESTION].\\nKeep\\nin\\nmind\\nthat\\nthis\\nresponse\\nmust\\nbe\\ndistinctive and one of\\nthe\\nexisting\\noptions\\nlisted\\nin\\noptions-\\n[ANSWER].\\nThis\\ninvolves\\nanalysis,\\ncognition,\\nand\\nthe\\ncapability to pinpoint\\ndistinctly\\npertinent\\ninformation.\\nC2\\nC1\\nquestion, and use LLM to generate knowledge that helps select the correct\\nanswer. The following Table 2 is an example of combining questions and\\nanswers to generate knowledge.\\nTable 2: An example of using LLM to generate knowledge about a relevant question.\\nPrompt\\nQuesiton\\nGood Answer\\nOutput\\nFrom the provided question [QUESTION]\\nand answer [GOOD ANSWER], please gener-\\nate a short piece of related knowledge. Your\\nresponse should be concise and provide rel-\\nevant information that pertains to the ques-\\ntion. Feel free to draw from various sources\\nand provide interesting and educational in-\\nsights related to the question.\\nAny\\ngood\\nplace\\nto\\nshop?\\nGucci?Thanks\\nfor your help\\nguys!\\ngo\\nto\\nVillaggio\\nVIP\\narea\\nGucci\\nis\\nthere\\nwith some other de-\\nsigner brands Talk to\\nmy crown\\nVillaggio\\nMall\\nis\\none\\nof\\nthe\\nmost\\npopular\\nshop-\\nping destinations in Doha,\\nQatar. It is home to a va-\\nriety of luxury brands, in-\\ncluding Gucci, Louis Vuit-\\nton, Prada, and more.\\n5. Experimental Setup\\nThis section includes data set information, training parameters, compar-\\nison experiments.\\n5.1. DataSet\\nThe two corpora we use to train and evaluate our model are SemEval2015\\nand SemEval2017 CQA datasets. The statistics of two corpora are shown in\\nTable 5.1.\\n10\\nTable 3: Statistical information of SemEval2015 and SemEval2017 Corpora.\\nSemEval2015\\nSemEval2017\\nStatistics\\nTrain\\nDev\\nTest\\nTrain\\nDev\\nTest\\nNumber of questions\\n2600\\n300\\n101\\n2660\\n500\\n293\\nNumber of answers\\n16541\\n1645\\n588\\n26690\\n5000\\n2930\\nAverage length of a question subject\\n5.80\\n5.53\\n7.47\\n5.40\\n5.34\\n5.76\\nAverage length of a question body\\n33.63\\n33.88\\n29.78\\n45.73\\n43.23\\n54.06\\nAverage length of an answer\\n31.07\\n29.36\\n25.40\\n37.34\\n35.46\\n39.50\\n5.2. Training and hyper parameters\\nWe exert NLTK toolkit to preprocess each question and its correspond-\\ning answers including capitalization conversion such as converting “CAR”\\ninto “car”, stemming such as transforming “working” to “work”, stop words\\nremoval such as removing “a” and “the”, etc. The algorithm we choose for\\noptimization is Adam Optimizer [34] with the momentum coefficient β 0.001.\\nThe per-minibatch L2 regularization parameter and batch size of the model\\nare set to 1×10−5 and 100 respectively, and the dropout value d is set to 0.3\\nto prevent overfitting. The maximum number of words in a question subject,\\na question body and an answer are set to 20, 110, 100, respectively. The\\nproposed model is implemented in Pytorch and the cross attention layer has\\na dimension of 300. We use the best parameters on development sets, and\\nevaluate the performance of our model on test sets.\\n5.3. Baseline\\nThe models compared with our proposed QAN model include: JAIST, us-\\ning SVM to incorporate various kinds of features by Tran et al. [35]. HITSZ-\\nICRC, proposing ensemble learning and hierarchical classification proposed\\nby Hou et al. [36]. Graph-cut, modeling the relationship between answers\\nin the same question thread proposed by Joty et al. [37]. FCCRF, applying\\nlocal learned classifiers and fully connected CRF proposed by Joty et al. [38].\\nBGMN, using the memory mechanism proposed by Wu et al. [39]. ECUN,\\na system contains three subtasks: Question-Comment Similarity, Question-\\nQuestion Similarity, and Question-External Comment Similarity proposed\\nby Wu et al. [40]. CNN-LSTM-CRF, proposing multilingual hierarchical\\nattention networks by Yang et al. [41]. And QCN, a Question Condensing\\nNetwork focusing on the similarity and disparities between question-subjects\\nand question-bodies proposed by Wu et al. [42].\\n11\\nFor LLM, We used LLaMA-7b-hf to generate relevant knowledge and the\\nfollowing large language model for answer selection: LLaMA-7b-hf.\\n5.4. Results and Analysis for PLMs\\nWe adopt the following three evaluation metrics,F1, Acc (accuracy), and\\nMAP (Mean Average of Precision) to compare the performance of QAN and\\nother current models with the results of comparison shown in Table 5.4.\\nTable 4: Comparisons of different models on two corpora.\\nDataset\\nModel\\nMAP\\nF1\\nAcc\\nSemEval2015\\n(1)JAIST\\nNA\\n0.7896\\n0.7910\\n(2)HITSZ-ICRC\\nNA\\n0.7652\\n0.7611\\n(3)Graph-cut\\nNA\\n0.8055\\n0.7980\\n(4)FCCRF\\nNA\\n0.8150\\n0.8050\\n(5)BGMN\\nNA\\n0.7723\\n0.7840\\n(6)CNN-LSTM-CRF\\nNA\\n0.8222\\n0.8224\\n(7)QCN\\nNA\\n0.8391\\n0.8224\\n(8)QAN (ours)\\nNA\\n0.8594\\n0.8465\\nSemEval2017\\n(9)ECUN\\n0.8672\\n0.7767\\n0.7843\\n(10)QCN\\n0.8851\\n0.7811\\n0.8071\\n(11)QAN(ours)\\n0.9286\\n0.8139\\n0.8385\\nFrom Table 5.4, it could be seen that our proposed model QAN outper-\\nforms all baseline models on three evaluation metrics (p < 0.05 based on\\nstudent t-test) with advancement attributed to the pre-trained BERT model\\nand attention mechanism. We use BERT model as pre-trained methods, fully\\nfusing context information in question subjects, question bodies and answers,\\nrespectively. Then cross attention between question subjects and answers as\\nwell as question bodies and answers helps our model estimate the relevance\\nof question-subject-answer pairs and question-body-answer pairs so as to ef-\\nfectively capture crucial interaction semantic features between questions and\\nanswers. QAN studies the relationship of questions and answers, fully cap-\\nturing semantic features at different angles, so that it greatly enhances the\\nperformance of the answer selection task.\\n5.4.1. Ablation Study\\nIn order to fully verify the improvement of our proposed model, we imple-\\nment six variants of QAN on the Yahoo! Answers dataset by ablation study.\\nThe description of ablation study is as follows:\\n12\\n• Without BERT but with task-specific word embeddings: Word embed-\\ndings are initialized with 300-dimensional GloVe trained on Wikipedia\\n2014 and Gigaword 5.\\n• Without BERT but with character embeddings: Word embeddings are\\ninitialized with 600-dimensional GloVe trained on a domain-specific\\nunannotated corpus.\\n• Without cross attention: Without cross attention between question\\nsubjects and answers as well as question bodies and answers.\\n• Without the interaction and prediction layer but with simple combi-\\nnation: Make simple combination of the outputs of the cross attention\\nlayer instead of the interaction and prediction layer.\\n• Without cross attention, Without the interaction and prediction layer\\nbut with simple combination: Only use BERT model to pre-train ques-\\ntion subjects and answers as well as question bodies and answers,then\\ncombine the ouputs as the final results.\\n• Without treat question subjects and question bodies separately: Treat\\nquestion subjects and question bodies as an entity to pre-train questions\\nand answers by BERT model.\\nTable 5: Ablation study of the seven models on the SemEval2017 dataset.\\nModel\\nMAP\\nF1\\nAcc\\n(1)w/o BERT but with task-specific word embeddings\\n0.8069\\n0.7165\\n0.7632\\n(2)w/o BERT but with character embeddings\\n0.7986\\n0.6859\\n0.7482\\n(3)w/o cross attention\\n0.9075\\n0.8019\\n0.8165\\n(4)w/o the interaction and prediction layer but with simple combina-\\ntion\\n0.8892\\n0.7839\\n0.8048\\n(5)w/o cross attention, w/o the interaction and prediction layer but\\nwith simple combination\\n0.8679\\n0.7642\\n0.7859\\n(6)w/o treat question subjects and question bodies separately\\n0.9126\\n0.8065\\n0.8275\\n(7)QAN (ours)\\n0.9286\\n0.8139\\n0.8385\\nThe results of the ablation study are shown in the Table 5.4.1. It can\\nbe seen that, whether it is lack of cross attention, separately treating, or\\ninteraction and prediction, in ablation or replacement of any component of\\nthe QAN model with another counterpart, the final results are all inferior to\\n13\\nthe result of full QAN model. Consequently, our proposed model QAN com-\\nprehensively learns the context features with BERT model, fully utilizes the\\nrelationship of questions and answers with attention mechanism and finally\\nintegrates semantic information of questions and answers to furthest enhance\\nthe performance of the answer selection task.\\n5.5. Results and Analysis for LLMs\\nWe use LLM to select the answer to the questions of dataset, and explore\\nthe knowledge enhancement, prompt optimization and so on.\\n5.5.1. Knowledge Augment\\nThe results in the Table 6 show that LLM can perform the answer se-\\nlection task well and achieve a good accuracy rate. At the same time, the\\nintroduction of external knowledge is obviously helpful in improving the ac-\\ncuracy of answer selection. After adding knowledge, LLM achieved higher\\naccuracy on both the SemEval2015 and SemEval2017 data sets. The quality\\nof knowledge affects the results of answer choices to a certain extent. The\\nfollowing Table 7 is an example of choosing the wrong answer originally, but\\nafter adding knowledge, the correct answer is chosen.It can be seen from the\\ntable that the generated knowledge has a certain correlation with the cor-\\nrect answer, which is conducive to choosing the correct answer. However,\\nin another example shown in Table 8, since the generated knowledge is not\\nvery relevant for answering the question, after adding knowledge, the correct\\nanswer is still not selected. In general, the knowledge introduced from out-\\nside, if it has some relevance to the question, will help LLM choose the right\\nanswer.\\nTable 6: The influence of knowledge on the correct rate of answer selection in large lan-\\nguage models.\\nDataset\\nModel\\nK\\nAcc\\nSemEval2015\\nLLaMA-7b-hf\\nw/o K\\n0.8617\\nw/ K\\n0.9255\\n0wen-14B-Chat\\nw/o K\\n0\\nw/ K\\n0\\nSemEval2017\\nLLaMA-7b-hf\\nw/o K\\n0.7725\\nw/ K\\n0.8275\\n0wen-14B-Chat\\nw/o K\\n0\\nw/ K\\n0\\n14\\nTable 7: An example where adding knowledge is helpful for answer selection.\\nQuestion\\nOptions\\nRight\\nOpiton\\nK\\nSelection\\nw/o K\\nSelection\\nw/ K\\nHi does any of you know where\\nI could get an England foot-\\nball\\nkit\\nfor\\nkids..\\nChecked\\nall sports shop in City Centre\\nand Olympic sports near crazy\\nsignal no luck.\\nIf you have\\nsee kids size kit please let me\\nknow.. Will try Villagio this af-\\nternoon..Thanks\\nC1:Have\\nseen\\nfooty\\nstrips\\nfor\\nkids\\nin\\nsports shop, opposite\\nDoha\\nClinic\\non\\nAl\\nMerqab Street.\\nC2:Thanks\\nBiddy\\nlou..will try that shop\\nalso..\\nC1\\nFootball kits for kids\\ncan be found in var-\\nious\\nsports\\nshops\\nin\\nDoha,\\nsuch\\nas\\nGO\\nSPORT\\nin\\nVillagio\\nand City Center.\\nC2\\nC1\\nTable 8: An example where adding knowledge is not helpful for answer selection.\\nQuestion\\nOptions\\nRigth\\nOpiton\\nK\\nSelection\\nw/o K\\nSelection\\nw/ K\\nHi Guys,Just wandering if\\nanyone on QL knows about\\nthis car or where i can get\\nthe cars engine computer\\nprogrammed.The\\none\\nwe\\nhave\\nis\\nthe\\noriginal\\nEleanor with ”ford racing”\\nengine.The\\nford\\nservice\\nshop\\nin\\nindustrial\\narea\\ndont seem to know any-\\nthing about this car.Any\\nhelp\\nwould\\nbe\\ngreatly\\nappretiated :)\\nC1:”won’t\\nmake\\nthe\\nengine\\nsmoke, unless you burnt a hole\\nin the piston, of something to\\nthat effect... if the engine had\\nan ECU system working at the\\ntime, if should not have hap-\\npened at all....”\\nC2:’nice ride...this is carbura-\\ntor or fuel injection?’\\nC3:’Try the threestar workshop\\nin industrial area street 24 -\\nnext to qatar technical inspec-\\ntion. But I would not drive this\\ncar - I just would put it in my\\nlivingroom and enjoy it....’\\nC3\\nThis\\ncar\\nhas\\na\\nFord\\nRacing\\nen-\\ngine.\\nC1\\nC2\\n5.5.2. Prompt Analysis\\nWe found that different prompts would result in different selections from\\nthe LLM. We used five different prompts to perform the LLM answer selec-\\ntion task in sequential order. After each selection with the current prompt,\\ncontinue the selection with the next prompt if the answer is not correct. The\\ncontent of each prompt optimization is shown in Table 9. The number of\\nquestions selected for each prompt and cumulative pair is shown in Figure 4.\\nAs shown in the Figure 4, we found that more questions selected correct an-\\nswers after optimizing prompt each time, indicating that optimizing prompt\\ncan improve the correct rate of answer selection. But the number of ques-\\ntions correctly selected by each prompt is at downward tendency. This may\\nbe due to the fact that the questions in the dataset vary in the difficulty\\nof selection, with the easy questions already having the correct answer se-\\nlected at the beginning. The difficult questions often need to be optimized\\nby prompt many times before the correct answer can be selected.\\nPrompt Length: We found that the length of prompts affect the choice\\nof answers, as shown in the Table 11 below, when the Prompt1 is too short,\\nimportant measures such as the relevance of the answer to the question\\nand the uniqueness of the answer may not be emphasized. Compared with\\n15\\nFigure 4: The number of questions selected correctly per prompt and the number of\\nquestions selected correctly cumulated\\nTable 9: Prompt Optimized Content.\\nPrompt\\nPrompt Optimized Content\\nPrompt1\\nNone\\nPrompt2\\nPrompt length is longer and the expression is clearer\\nPrompt3\\nOriginal questions and answers are placed at the top of the prompt\\nPrompt4\\nPrompt adds the original question subject\\nPrompt5\\nPut the task description at the beginning of prompt and prompt description is\\nalso simplified\\nPrompt1, Prompt2 is clearer and more detailed, and it also emphasizes the\\nform of answer users need to use, that is, only its digital identifier is pro-\\nvided, avoiding any additional information or feedback, thus improving the\\naccuracy of the instructions.\\nQuestion and Answer Location: We found that placing original ques-\\ntions and answers at the beginning of the prompt significantly improved the\\naccuracy of answer selection, as shown in Table 12. The questions, answers\\nand knowledge are placed at the beginning of prompt, and each of them\\nis displayed in a newline, which makes it easier for LLM to identify them.\\nTherefore, LLM can better combine questions and knowledge, measure the\\nrelevance of each answer, question and knowledge, and select the correct\\nanswer.\\nQuestion Subject: As shown in the Table 13, the inclusion of question\\nsubject in prompt also helps answer selection, possibly because the question\\nsubject supplement the text of the question meaning, help the LLM better\\nunderstand the task, and improve the accuracy of the correlation measure\\n16\\nTable 10: Prompt Specific Content.\\nPrompt\\nPrompt Specific Content\\nPrompt1\\nUtilizing the information in hint-[KNOWLEDGE], choose the serial number of the optimal response to the question-\\n[QUESTION] that is distinct from the options-[ANSWER].\\nPrompt2\\nGiven the discoveries from hint-[KNOWLEDGE], your task is to discern and specify the unique serial number tied to\\nthe most appropriate answer to question-[QUESTION]. Keep in mind that this response must be distinctive and one\\nof the existing options listed in options-[ANSWER]. This involves analysis, cognition, and the capability to pinpoint\\ndistinctly pertinent information.\\nPrompt3\\n”prompt-A”:”Question: QUESTION\\nBackground Knowledge: KNOWLEDGE\\nOptions: ANSWER\\nInstructions: Select the serial number of the most appropriate answer by considering its relevance, accuracy, and clarity.\\nYour response should be concise yet informative, demonstrating critical analysis skills and informed decision-making.\\nSupport your choice with specific details and evidence from both the question and background knowledge. Please note\\nthat it is essential not to reveal any personal or AI-related information in your response. Encourage creativity while\\nmaintaining accuracy in your answers.”\\n”prompt-B”:”You are an expert in answering multiple-choice questions and making informed decisions based on critical\\nanalysis and evidence. Your task is to select the most suitable answer from a list of options provided for a given question.\\nThe answer you choose should be concise, informative, and supported by specific details and evidence from both the\\nquestion and background knowledge. Remember not to disclose any personal or AI-related information in your response.\\nAim for creativity while maintaining accuracy in your answers.”\\nPrompt4\\nprompt-A:\\nQuestion Subject:SUBJECT\\nQuestion: QUESTION\\nBackground Knowledge: KNOWLEDGE\\nOptions: ANSWER\\nprompt-B:\\nYou are an expert in answering multiple-choice questions and making informed decisions based on critical analysis\\nand evidence. Your task is to select the most suitable answer from a list of options provided for a given question. The\\nanswer you choose should be concise, informative, and supported by specific details and evidence from both the question\\nand background knowledge. Remember not to disclose any personal or AI-related information in your response. Aim\\nfor creativity while maintaining accuracy in your answers.\\nInstructions:\\nSelect the serial number of the most appropriate answer from the above mentioned Options by considering its\\nrelevance, accuracy, and clarity. Your response should be concise yet informative, demonstrating critical analysis skills\\nand informed decision-making. Support your choice with specific details and evidence from the above mentioned Question\\nSubject and the above mentioned Question and the above mentioned Background Knowledge.\\nPlease note that it is\\nessential not to reveal any personal or AI-related information in your response. Encourage creativity while maintaining\\naccuracy in your answers.\\nPrompt5\\nprompt-A:\\nQuestion Subject:SUBJECT\\nQuestion: QUESTION\\nBackground Knowledge: KNOWLEDGE\\nOptions: ANSWER\\nprompt-B:\\nPlease select the serial number of the most appropriate answer from the given options based on its relevance,\\naccuracy, and clarity. Your response should be concise yet informative, demonstrating critical analysis skills and informed\\ndecision-making. Support your choice with specific details and evidence from the provided question subject, question,\\nand background knowledge. Make sure to prioritize accuracy in your response while also considering creative solutions\\nwhen applicable. Avoid including any personal or AI-related information in your answer.\\nTable 11: The effect of prompt length on results.\\nQuestion\\nOptions\\nRight\\nOption\\nSelection of\\nPrompt1\\nSelection of\\nPrompt2\\nCan anyone recommend a\\ngood hairdresser for men?\\ncheers!\\nC1:!!!!!!\\nnext\\nquestion;\\nplease!\\nC2:Patrice hair saloon in\\nVillaggio\\nor\\nLandmark\\nMall..but be warned;\\nits\\nnot cheap!\\nC2\\nC1\\nC2\\nand the selection rate of correct answers when measuring the correlation\\nbetween answers and questions.\\nThe Location of the Task Description: As shown in the Table 14,\\nplacing a description of the task, i.e., choosing the correct answer, at the\\nbeginning of the prompt description also helped with answer selection. This\\n17\\nTable 12: The effect of question and answers location on results.\\nQuestion\\nOptions\\nRight\\nOption\\nSelection of\\nPrompt2\\nSelection of\\nPrompt3\\nWith the world turning into a\\nsmall village; mixed marriages\\nhave become more and more\\ncommon. If you are in a mixed\\nmarriage relationship and liv-\\ning in Doha; Could you provide\\ndetails on how you and your\\nsignificant other met?\\nwhere?\\nhow long have you been mar-\\nried for?\\nand what are the\\ngoods and bads in a mixed mar-\\nriage? Thank you;\\nC1:Dear; there is no spe-\\ncific Formula for a success-\\nful Marriage whether its;\\nLoved;\\nArranged;\\nMixed;\\nIntercontinental its all de-\\npends on ones attitude n\\nsacrifices...\\nChoose your\\nway wiseLy gOOd lucK\\nC2:Met in a bar; must say\\nI was very drunk\\nC1\\nC2\\nC1\\nTable 13: The effect of question subject on results.\\nQuestion\\nOptions\\nRight\\nOption\\nSelection of\\nPrompt3\\nSelection of\\nPrompt4\\nHi I have just been offered a job in Qatar\\nand was wondering if you can tell me if this\\nis a good offer:\\n9200 basic salary (month)\\n5000 Accomodation (Month) 2000 Transport\\n(Month) 10000 Furniture allowance (once\\noff) I am a single guy from Australia; hop-\\ning to get a place of my own.\\nWould 5000\\ncover a fully furnished place or even an un-\\nfurnished 2 bedroom? What sort of car can\\ni buy for 2000 per month??\\nHope you can\\nhelp out thanks in advance..\\nC1:depends on what it is\\nfor\\n(responsibilities/govt\\nor pvt sector)\\nC2:that is a good offer for\\na single guy; we are 8 ppl\\nin the family and we live\\ngood with 8000 are month\\n. BUT on the end that de-\\npends on your life style .\\nC2\\nC1\\nC2\\nmay be because the task description is the important information of the\\nprompt.\\nPutting the task description at the beginning can help LLM to\\nquickly locate important information, simplify some unnecessary content in\\nthe prompt, and make the structure of the prompt clearer. These factors\\nhelp LLM to better understand the meaning of the prompt, so as to choose\\nthe correct answer.\\nTable 14: The effect of the location of the task description on results.\\nQuestion\\nOptions\\nRight\\nOption\\nSelection of\\nPrompt4\\nSelection of\\nPrompt5\\nis 3rd cousin relationship\\nalready accepted in church\\nor in society?\\ni mean like\\n””lovers””...”\\nC1:Even\\nthe\\nthought\\nof\\nit\\nis\\na\\nsin...yaiiiks!!!\\nC2:pit;\\nit\\noccurs\\nmore\\nwith\\n1st\\ncousins... and abnormal birth? its\\ncommonly caused by the pressure\\ngiven to the mother\\nC1\\nC2\\nC1\\nWe have optimized the prompt in four aspects: the length of the prompt,\\nthe position of the answer to the question, the topic of the question, and the\\nposition of the task description. The results show that the optimized prompt\\nhelps LLM select the right answer, so more questions could be selected for\\nthe right answer.\\n18\\n6. Conclusion\\nAnswer selection is the core part of CQA. To automate the answer selec-\\ntion progress, we propose QAN, Question-Answer Cross Attention Networks\\nWith Pre-trained models for Answer Selection.\\nWe use BERT model to\\ncapture context features of question subjects, question bodies and answers,\\nrespectively. With cross attention mechanism between question subjects and\\nanswers as well as question bodies and answers, we obtain comprehensive\\ninteractive information between questions and answers. Through integrat-\\ning attention-questions and attention-answers, QAN model gets final results\\nwhich achieve state-out-of-art performance. In the future, our research group\\nwill test QAN model in more fields to increase the universality of it and man-\\nage to improve the computing speed of QAN model to further level up the\\nperformance of our solution.\\nAs the large language model is now a hot research spot, we also uses the\\nlarge language model to select the correct answer, combined with knowledge\\nenhancement. The result shows that the introduction of external knowledge\\ncan improve the accuracy of LLM in selecting the answer. At the same time,\\nthe prompt input to LLM was optimized from the aspects of prompt length,\\nquestion and answer position, question subject and task description position.\\nThe result showed that after the optimization of prompt, the correct rate of\\nselecting answers could be improved, and a good result was obtained. In the\\nfuture, prompt will be optimized and analyzed in more aspects to further\\nimprove LLM performance on correct answer selection tasks.\\nReferences\\n[1] X. Zhang, S. Li, L. Sha, H. Wang, Attentive interactive neural networks\\nfor answer selection in community question answering, in: AAAI Con-\\nference on Artificial Intelligence, 2017, p. 3525–3531.\\nURL https://api.semanticscholar.org/CorpusID:6423645\\n[2] P. K. Roy, S. Saumya, J. P. Singh, S. Banerjee, A. Gutub, Analysis\\nof community question-answering issues via machine learning and deep\\nlearning: State-of-the-art review, CAAI Transactions on Intelligence\\nTechnology 8 (1) (2023) 95–117.\\n[3] B. Zhang, H. Wang, L. Jiang, S. Yuan, M. Li, A novel bidirectional\\nlstm and attention mechanism based neural network for answer selection\\n19\\nin community question answering., Computers, Materials & Continua\\n62 (3) (2020).\\n[4] W. Zhang, Z. Chen, C. Dong, W. Wang, H. Zha, J. Wang, Graph-based\\ntri-attention network for answer ranking in cqa, ArXiv abs/2103.03583\\n(2021).\\nURL https://api.semanticscholar.org/CorpusID:232135063\\n[5] B. Jin, E. Chen, H. Zhao, Z. Huang, Q. Liu, H. Zhu, S. Yu, Promo-\\ntion of answer value measurement with domain effects in community\\nquestion answering systems, IEEE Transactions on Systems, Man, and\\nCybernetics: Systems 51 (5) (2021) 3068–3079.\\ndoi:10.1109/TSMC.\\n2019.2917673.\\n[6] T. T. Ha, A. Takasu, T. C. Nguyen, K. H. Nguyen, V. N. Nguyen,\\nK. A. Nguyen, S. G. Tran, Supervised attention for answer selection in\\ncommunity question answering, IAES International Journal of Artificial\\nIntelligence 9 (2) (2020) 203.\\n[7] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al., Large language\\nmodels encode clinical knowledge, Nature (2023) 1–9.\\n[8] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,\\nS. Pfohl, H. Cole-Lewis, D. Neal, et al., Towards expert-level med-\\nical question answering with large language models, arXiv preprint\\narXiv:2305.09617 (2023).\\n[9] V. Li´evin, C. E. Hother, O. Winther, Can large language models reason\\nabout medical questions?, arXiv preprint arXiv:2207.08143 (2022).\\n[10] M. Zaki, N. Krishnan, et al., Mascqa: A question answering dataset\\nfor investigating materials science knowledge of large language models,\\narXiv preprint arXiv:2308.09115 (2023).\\n[11] Z. Shao, Z. Yu, M. Wang, J. Yu, Prompting large language models with\\nanswer heuristics for knowledge-based visual question answering (2023).\\narXiv:2303.01903.\\n[12] C. Shah, J. Pomerantz, Evaluating and predicting answer quality in\\ncommunity qa, in: Proceedings of the 33rd international ACM SIGIR\\n20\\nconference on Research and development in information retrieval, 2010,\\npp. 411–418.\\n[13] K. Zhang, W. Wu, H. Wu, Z. Li, M. Zhou, Question retrieval with high\\nquality answers in community question answering, in: Proceedings of\\nthe 23rd ACM international conference on conference on information\\nand knowledge management, 2014, pp. 371–380.\\n[14] Z. Zhao, L. Zhang, X. He, W. Ng, Expert finding for question answering\\nvia graph regularized matrix completion, IEEE Transactions on Knowl-\\nedge and Data Engineering 27 (4) (2014) 993–1004.\\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\n L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017).\\n[16] W. Guo, Y. Zhang, J. Yang, X. Yuan, Re-attention for visual question\\nanswering, IEEE Transactions on Image Processing 30 (2021) 6730–\\n6743.\\n[17] X. Zheng, B. Wang, X. Du, X. Lu, Mutual attention inception net-\\nwork for remote sensing visual question answering, IEEE Transactions\\non Geoscience and Remote Sensing 60 (2021) 1–14.\\n[18] Q. Guo, S. Cao, Z. Yi, A medical question answering system using\\nlarge language models and knowledge graphs, International Journal of\\nIntelligent Systems 37 (11) (2022) 8548–8564.\\n[19] W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nM. Jiang, Generate rather than retrieve: Large language models are\\nstrong context generators, arXiv preprint arXiv:2209.10063 (2022).\\n[20] D. Lee, T. Whang, C. Lee, H. Lim, Towards reliable and fluent large\\nlanguage models: Incorporating feedback learning loops in qa systems,\\narXiv preprint arXiv:2309.06384 (2023).\\n[21] Y. Weng, B. Li, F. Xia, M. Zhu, B. Sun, S. He, K. Liu, J. Zhao, Large\\nlanguage models need holistically thought in medical conversational qa,\\narXiv preprint arXiv:2305.05410 (2023).\\n21\\n[22] L. Cai, G. Zhou, K. Liu, J. Zhao, Large-scale question classification\\nin cqa by leveraging wikipedia semantic knowledge, in: Proceedings of\\nthe 20th ACM International Conference on Information and Knowledge\\nManagement, CIKM ’11, Association for Computing Machinery, New\\nYork, NY, USA, 2011, p. 1321–1330. doi:10.1145/2063576.2063768.\\nURL https://doi.org/10.1145/2063576.2063768\\n[23] F. Jing, H. Ren, W. Cheng, X. Wang, Q. Zhang, Knowledge-enhanced\\nattentive learning for answer selection in community question answering\\nsystems, Knowledge-Based Systems 250 (2022) 109117.\\n[24] W. Huang, Q. Qu, M. Yang, Interactive knowledge-enhanced attention\\nnetwork for answer selection, Neural Computing and Applications 32\\n(2020) 11343–11359.\\n[25] Z. Li, B. Zou, Y. Fan, Y. Hong, Kepr: Knowledge enhancement and\\nplausibility ranking for generative commonsense question answering,\\narXiv preprint arXiv:2305.08347 (2023).\\n[26] V.-T. Nguyen, A.-C. Le, H.-N. Nguyen, A model of convolutional neural\\nnetwork combined with external knowledge to measure the question sim-\\nilarity for community question answering systems, International Journal\\nof Machine Learning and Computing 11 (3) (2021) 194–201.\\n[27] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding (2019).\\narXiv:1810.04805.\\n[28] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, Y. Bengio,\\nA structured self-attentive sentence embedding, ArXiv abs/1703.03130\\n(2017).\\nURL https://api.semanticscholar.org/CorpusID:15280949\\n[29] C. Q, Z. X, L. Z, et al., Enhanced lstm for natural language inference[c],\\nin: meeting of the association for computational linguistics 2017, 2017,\\npp. 1657–1668.\\n[30] M. L, M. R, L. G, et al., Natural language inference by tree-based con-\\nvolution and heuristic matching[c], in: meeting of the association for\\ncomputational linguistics 2016, 2016, pp. 130–136.\\n22\\n[31] G. Balikas, John-arthur at SemEval-2023 task 4: Fine-tuning large lan-\\nguage models for arguments classification, in: A. K. Ojha, A. S. Do˘gru¨oz,\\nG. Da San Martino, H. Tayyar Madabushi, R. Kumar, E. Sartori (Eds.),\\nProceedings of the 17th International Workshop on Semantic Evaluation\\n(SemEval-2023), Association for Computational Linguistics, Toronto,\\nCanada, 2023, pp. 1428–1432.\\ndoi:10.18653/v1/2023.semeval-1.\\n197.\\nURL https://aclanthology.org/2023.semeval-1.197\\n[32] L. Zhou, K. Small, Multi-domain dialogue state tracking as dynamic\\nknowledge graph enhanced question answering (2020).\\narXiv:1911.\\n06192.\\n[33] N. Bian, X. Han, B. Chen, L. Sun, Benchmarking knowledge-enhanced\\ncommonsense question answering via knowledge-to-text transformation\\n(2021). arXiv:2101.00760.\\n[34] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization\\n(2017). arXiv:1412.6980.\\n[35] Q. H. Tran, D. V. Tran, T. Vu, M. L. Nguyen, S. B. Pham, Jaist:\\nCombining multiple features for answer selection in community ques-\\ntion answering, in: Proceedings of the 9th International Workshop on\\nSemantic Evaluation (SemEval-2015) 2015, Denver, Colorado, 2015, p.\\n215–219.\\n[36] Y. Hou, C. Tan, X. Wang, Y. Zhang, J. Xu, Q. Chen, Hitsz-icrc: Ex-\\nploiting classification approach for answer selection in community ques-\\ntion answering, in: Proceedings of the 9th International Workshop on\\nSemantic Evaluation (SemEval-2015) 2015, Denver, Colorado, 2015, p.\\n196–202.\\n[37] S. Joty, A. Barr´on-Cede˜no, G. D. S. Martino, L. M. Simone Filice,\\nA. Moschitti, P. Nakov, Global thread-level inference for comment clas-\\nsification in community question answering, in: Proceedings of the 2015\\nConference on Empirical Methods in Natural Language Processing 2015,\\nLisbon, Portugal, 2015, p. 573–578.\\n[38] S. Joty, L. M`arquez, P. Nakov, Joint learning with global inference for\\ncomment classification in community question answering, in: Proceed-\\n23\\nings of the 2016 Conference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Language Technologies\\n2016, San Diego, California, 2016, p. 703–713.\\n[39] W. Wu, H. Wang, S. Li, Bi-directional gated memory networks for an-\\nswer selection, in: Chinese Computational Linguistics and Natural Lan-\\nguage Processing Based on Naturally Annotated Big Data 2017, 2017,\\np. 251–262.\\n[40] G. Wu, Y. Sheng, M. Lan, Y. Wu, Ecnu at semeval2017 task 3: Using\\ntraditional and deep learning methods to address community question\\nanswering task, in: Proceedings of the 11th International Workshop on\\nSemantic Evaluation (SemEval-2017) 2017, 2017, p. 365–369.\\n[41] Y. Xiang, X. Zhou, Q. Chen, et al., Incorporating label dependency for\\nanswer quality tagging in community question answering via cnn-lstm-\\ncrf, in: COLING 2016, 26th International Conference on Computational\\nLinguistics, Proceedings of the Conference 2016, Osaka, Japan, 2016, p.\\n1231–1241.\\n[42] W. W, S. X, W. H, et al., Question condensing networks for answer se-\\nlection in community question answering, in: meeting of the association\\nfor computational linguistics 2017, 2018, pp. 1746–1755.\\n24\\n'),\n",
       " Document(metadata={'Published': '2024-03-28', 'Title': 'JDocQA: Japanese Document Question Answering Dataset for Generative Language Models', 'Authors': 'Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe', 'Summary': 'Document question answering is a task of question answering on given\\ndocuments such as reports, slides, pamphlets, and websites, and it is a truly\\ndemanding task as paper and electronic forms of documents are so common in our\\nsociety. This is known as a quite challenging task because it requires not only\\ntext understanding but also understanding of figures and tables, and hence\\nvisual question answering (VQA) methods are often examined in addition to\\ntextual approaches. We introduce Japanese Document Question Answering (JDocQA),\\na large-scale document-based QA dataset, essentially requiring both visual and\\ntextual information to answer questions, which comprises 5,504 documents in PDF\\nformat and annotated 11,600 question-and-answer instances in Japanese. Each QA\\ninstance includes references to the document pages and bounding boxes for the\\nanswer clues. We incorporate multiple categories of questions and unanswerable\\nquestions from the document for realistic question-answering applications. We\\nempirically evaluate the effectiveness of our dataset with text-based large\\nlanguage models (LLMs) and multimodal models. Incorporating unanswerable\\nquestions in finetuning may contribute to harnessing the so-called\\nhallucination generation.'}, page_content=\"JDocQA: Japanese Document Question Answering\\nDataset for Generative Language Models\\nEri Onami1,2, Shuhei Kurita2, Taiki Miyanishi3, Taro Watanabe1\\n1Nara Institute of Science and Technology, 2RIKEN, 3ATR\\n{onami.eri.ob6, taro}@is.naist.jp, shuhei.kurita@riken.jp, miyanishi@atr.jp\\nAbstract\\nDocument question answering is a task of question answering on given documents such as reports, slides, pamphlets,\\nand websites, and it is a truly demanding task as paper and electronic forms of documents are so common in\\nour society. This is known as a quite challenging task because it requires not only text understanding but also\\nunderstanding of figures and tables, and hence visual question answering (VQA) methods are often examined in\\naddition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale\\ndocument-based QA dataset, essentially requiring both visual and textual information to answer questions, which\\ncomprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each\\nQA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate\\nmultiple categories of questions and unanswerable questions from the document for realistic question-answering\\napplications. We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs)\\nand multimodal models. Incorporating unanswerable questions in finetuning may contribute to harnessing the\\nso-called hallucination generation.\\nKeywords: Multimodal Document Processing, Question Answering, Natural Language Generation\\n1.\\nIntroduction\\nA thorough understanding of documents that are\\ncomposed of both texts and graphical elements\\nsuch as slides, reports, webpages, and pamphlets\\nis essential for intelligent agents that process mul-\\ntimedia documents and answer some questions\\non such documents. Document visual understand-\\nings have been studied to achieve joint understand-\\nings of textual and visual elements in such docu-\\nments or images, including bookcovers (Mathew\\net al., 2021a), scene images with characters (Singh\\net al., 2019), webpages (Tanaka et al., 2021), ta-\\nbles (Smock et al., 2022) and slides (Tanaka et al.,\\n2023). These datasets have received significant\\nattention as documents are a common form in vari-\\nous industrial, public, and private sectors in the En-\\nglish domain. It is also notable that the document\\nvisual question answering tasks are still quite diffi-\\ncult despite its significance in industries because\\nthey heavily rely on both textual and visual modali-\\nties as the documents often include complex visual\\nalignments of texts on figures, charts and illustra-\\ntions. Especially in document question answering,\\nmodels are required to connect multiple modali-\\nties to figure out answers. There are quite limited\\ndatasets in which both visual and textual informa-\\ntion is required to answer questions on documents.\\nIt is also a problem that despite the significance of\\nthese tasks, the primary focus of these datasets\\nis limited to the English domain and dataset con-\\nstructions on other languages are still limited. As\\na document question-answering task, Japanese\\ndocuments have several characteristics compared\\nto English documents. One of the major difficulties\\nin Japanese document processing lies in the two\\nofficial writing styles in Japanese: one is a left-to-\\nright horizontal style and the other is an upside-to-\\nbottom vertical style, which requires both writing\\nstyle comprehension in the dataset.\\nThere has been significant progress in the gen-\\nerative large language models (LLMs) and multi-\\nmodal models these days. GPT-4 (OpenAI, 2023)\\nallowing zero-shot applications in both language\\nrelated and even in multimodal tasks.\\nInstruct-\\nBLIP (Dai et al., 2023) takes both textual and im-\\nage inputs and generates texts such as image cap-\\ntions or visual question answering following tex-\\ntual prompts. The success of LLMs also triggers\\nthe competitive development of several publicly-\\navailable LLMs in Japanese. Instruction tuning of\\nLLM can improve its ability to adhere to certain\\ndomains or usage, rendering them more suitable\\nfor particular applications rather than maintaining\\nits ability in a general understanding of language\\nand limited expertise (Mishra et al., 2022; Sanh\\net al., 2022). While there have been numerous at-\\ntempts to fine-tune instruction for highly technical\\nand professional adaptation (Ouyang et al., 2022;\\nWei et al., 2022), there is still a lack of adequately\\nprepared high-quality visual question and answer-\\ning datasets that can be used for generative lan-\\nguage model-based question answering and par-\\nticularly developed outside of the English domain.\\nTo address the demand for a large-scale and fully\\nannotated Japanese document question answer-\\ning dataset, we introduce a JDocQA dataset by\\ncollecting Japanese documents in PDF styles from\\narXiv:2403.19454v1  [cs.CL]  28 Mar 2024\\nQ: 名古屋駅から桜⼭キャンパスへ地下鉄で⾏くには\\nどの路線を使ったらよいですか。/ Which subway \\nline should I take to get from Nagoya \\nStation to the Sakura Mountain Campus?\\nQ: ピアが、3時間以上の会議に1回出席し、シドニー\\nに⼀泊する場合、審査パネル会議への参加に対する\\n出席費と滞在費として受け取る⾦額はいくらになりま\\nすか︖朝⾷、昼⾷、⼣⾷を1回ずつと諸費⽤も含む\\nとします。/ How much would cost the \\nbusiness trip budget when go to Sydney, \\nstaying 1 night attending MTG, including \\nmeal and other expenses?\\nQ: NIIが開発したファイル転送プロトコルMMCFTPを⽤い\\nて東京ーデンバー間で転送実験をした図によると、⻘⾊で\\n結ばれているのはどのような国際通信ですか︖\\nAccording to the diagram illustrating the \\ntransfer experiment between Tokyo and \\nDenver using the file transfer protocol \\nMMCFTP developed by NII, what does the \\nblue line represent in terms of international \\ncommunication?\\nQ: 富⼠市ではシルバー⼈材センターを設けています\\nか。/ Does Fuji city have elderly people \\nemployment support center?\\nA: 図上で⻘⾊で⽰されている国際通信は、東京から⾹\\n港を経由しシンガポールまでJGN/SingARENというネット\\nワークで通信し、シンガポールからロサンジェルスを経由して\\nデンバーまでinternet2/SingARENというネットワークで\\n通信します。/ The blue lines on the diagram \\nrepresent international communication that \\ntravels from Tokyo to Denver. Specifically, it \\ngoes through Hong Kong, then Singapore, \\nusing the JGN/SingAREN network, and from \\nSingapore to Los Angeles, eventually reaching \\nDenver via the internet2/SingAREN network.\\nA: 桜通線/ Sakura-dori Line. \\nA: いいえ/ No.\\nA: 1330豪ドルです/ 1330 AUD.\\n(Caluculation of 415+165+750=1330)\\n(1) Yes/No\\n(2) Factoid\\n(3) Numerical\\n(4) Open-ended\\nFigure 1: JDocQA sample question answering instances in four question categories with the annotated\\nbounding boxes of the supporting facts in red color.\\nopen-access sources including multiple formats\\nof documents: slides, reports, websites and pam-\\nphlets and manually annotating question-answer\\npairs on them. JDocQA consists of 11,600 question\\nand answer pairs on the collected 5,504 documents\\nas references for answering the question, four dif-\\nferent question categories and 1,000 multi-page\\nquestions. Each question is designed to refer to\\nboth textual and visual components such as tables\\nor figures by annotators. We also introduce unan-\\nswerable questions: questions that have no answer\\nclues in the referenced documents. In experiments,\\nwe first present the effectiveness of finetuning LLM\\nwith our dataset. We also suggest that incorporat-\\ning these unanswerable questions can contribute\\nto mitigating hallucination, which is often observed\\nduring the generation by LLMs.\\n2.\\nRelated Work\\nMultimodal question answering datasets.\\nVi-\\nsual question answering is a task of question-\\nanswering given visual contexts such as images fol-\\nlowing textual queries (Malinowski and Fritz, 2014;\\nAntol et al., 2015). Earlier VQA studies have been\\nnot limited to images alone but cover various forms\\nof media such as textbook (Kembhavi et al., 2017),\\nrecipe (Yagcioglu et al., 2018), comic book (Iyyer\\net al., 2016), movie (Tapaswi et al., 2016). Among\\nthem, a document VQA, which is a task designed\\nfor text embedded in real-world images, has at-\\ntracted a lot of attention toward a comprehensive\\nunderstanding of documents from both the visual\\nand textual sides. Currently, some useful docu-\\nment VQA datasets have been published, such as\\nOCR-VQA (Mishra et al., 2019), TextVQA (Singh\\net al., 2019), and DocVQA (Mathew et al., 2021a),\\nVisualMRC (Tanaka et al., 2021), WebSRC (Chen\\net al., 2021), and InfographicVQA (Mathew et al.,\\n2022). Most of these studies concentrate on the\\nsingle-image VQA where each question-answer\\npair has a single relevant image that always in-\\nclude sufficient information for question-answering.\\nUnlike the single-image VQA, the ability to com-\\nprehend multiple pages or charts to answer ques-\\ntions is more practical for understanding the slides\\nand documents people read in the daily work. To\\ntackle such multi-image VQA, MultiModalQA (Tal-\\nmor et al., 2021), MP-DocVQA (Tito et al., 2022)\\nand SlideVQA (Tanaka et al., 2023) concentrate\\non the multi-hop and numerical reasoning while\\nconsidering multimodal context similar to previous\\nworks (Rajpurkar et al., 2016; Yang et al., 2018;\\nDua et al., 2019). It is also notable that in docu-\\nment question answering Tang et al. (2023) pro-\\nposed Universal Document Processing (UDOP),\\nunifying vision, text, and layout of the input docu-\\nment through vision-text-layout Transformer.\\nText-based question answering in Japanese.\\nSome related tasks of text-based question answer-\\ning for Japanese have been studied (Miyazaki and\\nShimizu, 2016; Yanaka and Mineshima, 2022; Taka-\\nhashi et al., 2019; Kurihara et al., 2022). Miyazaki\\nand Shimizu (2016) created a Japanese image cap-\\ntioning dataset, which is the Japanese version of\\nthe MS-COCO captions dataset, and demonstrated\\nthat using both bilingual datasets outperforms us-\\nCategory\\nDocuments\\nQA\\n(1) Yes/No\\n(2) Factoid\\n(3) Numerical\\n(4) Open-ended\\nMulti-page\\nUnanswerable\\nPamphlet\\n1,715\\n4,025\\n605\\n748\\n660\\n2,012\\n46\\n671\\nSlide\\n1,640\\n3,276\\n545\\n593\\n507\\n1,631\\n448\\n449\\nReport\\n2,086\\n4,167\\n703\\n687\\n693\\n2,084\\n506\\n668\\nWebsite\\n67\\n132\\n2\\n24\\n6\\n100\\n0\\n0\\nTotal\\n5,504\\n11,600\\n1,855\\n2,052\\n1,866\\n5,827\\n1,000\\n1,788\\nTable 1: Number of document styles and question-answer pairs by the four question categories, multi-\\npage and unanswerable questions.\\nCategory\\n(1) Y/N\\n(2) Fact.\\n(3) Num.\\n(4) Open.\\nContext\\n963.81\\n1036.63\\n1020.04\\n1017.25\\nQuestion\\n67.75\\n61.26\\n60.36\\n65.44\\nAnswer\\n3.77\\n16.01\\n8.22\\n65.97\\nTable 2: Average character length.\\ning monolingual ones.\\nYanaka and Mineshima\\n(2022) introduced a Japanese textual entailment\\ndataset and highlighted that many existing models\\nthat have focused on English do not adequately ac-\\ncount for Japanese language characteristics. Taka-\\nhashi et al. (2019) proposed a QA dataset based\\non Japanese blogs related to driving, with the aim\\nof creating a model that can understand the mean-\\ning of sentences or texts in Japanese. It is also\\nnotable that Miyao and Kawazoe (2013) and rel-\\nevant Todai Robot Project1 arranged the dataset\\nof Japanese University entrance-exams. In math\\nand physics subjects, their dataset includes limited\\nmultimodal contents in DTD file format, although\\nit doesn’t cover general domains as of JDocQA.\\nAmong the datasets related to the Japanese lan-\\nguage, JGLUE (Kurihara et al., 2022) is similar to\\nour work in terms of the aim of the datasets. JGLUE\\nis a large-scale natural language understanding\\n(NLU) benchmark purposed for the evaluation of\\nLLM. It includes various tasks, such as text clas-\\nsification, sentence pair classification, and QA to\\nassess Japanese comprehension. In contrast to\\nJGLUE, our dataset offers a diverse range of ques-\\ntion types which can be useful for instruction tuning\\nand contains both Japanese text and image data,\\nwhich can be used for multimodal models. Another\\nkey difference with JGLUE is that JDocQA incor-\\nporates unanswerable questions to help suppress\\nhallucinations.\\n3.\\nDataset\\n3.1.\\nTask Overview and Formulation\\nWe consider generative question answering where\\na model generates a textual answer following the\\ndocument context and textual question. For realis-\\ntic applications of a wide range of user questions\\nfor documents, we prepare four categories of ques-\\ntions: (1) yes/no, (2) factoid, (3) numerical, and\\n(4) open-ended. In yes/no questions, answers\\n1https://21robot.org/index-e.html\\nFigure 2: The number of visual information cate-\\ngories.\\nare “yes” or “no.” In factoid questions, answers\\nare some facts, such as named entities, that typi-\\ncally appear in the given documents. In numerical\\nquestions, answers are numeric values, often in-\\ncluding some numerals (some units, e.g., km or\\nJapanese numerals such as “個(objects)” and “人\\n(persons)”). These numeric values are written in\\nthe documents or are calculated from other num-\\nbers in the documents. In open-ended questions,\\nfree-form responses are required. For such ques-\\ntions, we aim to assess complex comprehension\\nabilities, such as the ability to form opinions or brief\\nexplanations based on the provided contexts and\\nquestions. Figure 1 presents samples of these four\\ncategories of questions. All examples include di-\\nverse images and question types related to some\\nJapanese documents collected. We also include\\nunanswerable questions for each question cate-\\ngory.\\nIn the realistic applications of the question an-\\nswering, no answers can be found in the referenced\\ndocument. Therefore, it is expected that the correct\\nresponses for such questions are “not mentioned\\nin the text.” The prediction of the unanswerable\\nquestions is not addressed in previous Japanese\\nquestion answering datasets such as Kurihara et al.\\n(2022).\\n3.2.\\nDataset Statistics\\nJDocQA dataset comprises 5,504 files and 11,600\\nquestion-and-answer pairs in Japanese. The statis-\\ntics of categorized question types are as follows:\\n(1) yes/no questions: 1,855, (2) factoid questions:\\nDataset\\n#Questions\\n#Images\\n#BBoxes\\nLanguage\\nMultihop\\nOCR-VQA (Mishra et al., 2019)\\n1002k\\n207k\\n-\\nEnglish\\n-\\nDocVQA (Mathew et al., 2021a)\\n50k\\n12k\\n-\\nEnglish\\n-\\nInfographicVQA (Mathew et al., 2022)\\n5.9k\\n30k\\n-\\nEnglish\\n-\\nMP-DocVQA (Tito et al., 2022)\\n46k\\n48k\\n-\\nEnglish\\n✓\\nSlideVQA (Tanaka et al., 2023)\\n14.5k\\n52k\\n890k\\nEnglish\\n✓\\nJDocQA (Ours)\\n11.6k\\n268k\\n11k\\nJapanese\\n✓\\nTable 3: The comparison of the document question answering datasets.\\n2,052, (3) numerical questions: 1,866, (4) open-\\nended questions: 5,827. Additionally, 1,788 ques-\\ntions require referencing multiple pages to answer,\\nand in 1,000 questions the correct answer is not\\nmentioned in the text, as shown in Table 1. Some\\nPDF documents contain both slide and report for-\\nmats within the same documents. For such doc-\\numents, we count them in both categories of the\\nslide and report formats when we calculate the total\\nnumber of the documents2. Table 2 represents the\\naverage length of the context, question, and answer\\nin our dataset, and Figure 2 shows the category of\\nthe visual information referenced by question or an-\\nswer in our dataset. The comparison of document\\nquestion answering datasets are shown in Table 3.\\n3.3.\\nDataset Creation\\nThe overall dataset creation and annotation pro-\\ncess is presented in Figure 3.\\nPDF collection.\\nWe gather public documents,\\nsuch as, municipality pamphlets and websites, that\\nare created by Japanese governmental agencies\\nor local governments. We manually collected PDF\\ndocuments from open-access resources such as\\nJapanese National Diet Library (NDL)’s digital col-\\nlection, web archive projects (WARP)3 and web-\\nsites of Japanese government ministries. We man-\\nually gathered documents such as reports, pam-\\nphlets or websites that are published by public or\\nquasi-public sectors, such as local governments or\\npublic universities through WARP. We also gather\\nJapanese ministry documents such as slides and\\nreports from their websites following the govern-\\nment agencies’ policies. Those documents cover\\na wide range of topics, for instance, economic poli-\\ncies, education policies, labor issues, health and\\nhygiene, agriculture, forestry, fisheries, culture and\\narts, history, related to governmental policy or pol-\\nicy guidelines, as well as the everyday affairs of\\nlocal governments. These documents also include\\nvisual elements such as figures, tables, charts, pic-\\ntures, or mandala charts, complex figures with a\\ncombination of texts and objects typically seen in\\nthe Japanese public administrative sector’s official\\n2The number of total documents is not the sum of\\nthe number of the subcategories in Table 1 due to this\\ncounting.\\n3https://warp.ndl.go.jp/\\n(1) Collect documents\\n(2) Extract PDF text and normalize \\nQ: How much was the GDP \\ngrowth between 2014-2016?\\nA:2.6 billion \\n(i)\\nExtract embeded text\\n(ii) OCR text\\n… This line chart \\nshows the growth \\nof GDP and the \\nbar chart shows \\nthe amount of \\ninvestment in IT \\nsector… \\n2014\\n2016\\n1 billion\\n3.6 billion\\n(3) Annotate QA\\nQA must refer to both visual \\nand textual element\\nFigure 3: Annotation process.\\ndocument. We classify these documents into four\\ncategories, namely, pamphlet, slide, report, and\\nwebsite considering the form of the documents.\\nText Extraction & Normalization.\\nWe extracted\\ntexts from PDF documents with PyPDF24. We also\\nnotice that some PDF documents are probably cre-\\nated from paper scans, and we cannot extract em-\\nbedded texts from such documents. Therefore, we\\nextracted texts from the document page images\\nby OCR (Optical Character Recognition) as an al-\\nternative source. After the text extraction or OCR,\\nwe removed mistakenly recognized symbols and\\nemojis, or duplicated characters from texts when\\nthe same character continuously and repeatedly\\nappeared more than five times.\\nAnnotation Procedure.\\nWe ask 43 annotators\\nin total for the question-answering pairs annotation\\non documents. As documents include rich textual\\nand visual elements (e.g., graphs, charts, maps,\\nillustrations, and a mix of vertical and horizontal\\nwritten text), we made question answer pairs that\\nare related to both textual and visual information.\\nWe ask annotators to write up two to four question-\\nanswer annotations in each document. We also\\nask not to use any AI-tools such as OpenAI Chat-\\nGPT during the annotation process. Each ques-\\ntion is accompanied with the supporting facts as\\nmarked in red in Figure 1 and Figure 3. We classify\\na subset of questions that have multiple support-\\ning facts in multiple pages as multi-page questions.\\nMulti-page questions are considerably difficult from\\ntheir single-page counterparts. For unanswerable\\nquestions, we ask annotators to write questions\\n4We also examined PyMuPDF. However, the quality\\nof the extracted texts was not changed greatly.\\nthat lack supporting facts in the documents, mak-\\ning them impossible to answer based on the given\\ndocuments.\\nVisual inputs and bounding boxes.\\nWe pre-\\npared three types of images for visual inputs for\\nmultimodal models. The first type of images are\\nthose of the whole page of the documents including\\nthe annotated question answering pairs. The sec-\\nond type of images are those cropped by bounding\\nboxes on which annotators based their answers\\nsuch as tables or figures of the pages.\\nWhen\\nmultiple bounding boxes are annotated to a sin-\\ngle question-answer pair, multiple cropped images\\nare combined together into a single image here.\\nThe third type of images are blank (white) images\\nthat are used for ablation studies.\\n4.\\nExperiments\\n4.1.\\nQuestion-Answering Task\\nOur dataset aims to evaluate the question answer-\\ning ability following the document contexts, includ-\\ning textual and visual information, and questions\\nvia open-ended text generation. As discussed in\\nSec. 3.1, our dataset consists of four forms of ques-\\ntions: yes/no5, factoid, numerical, and open-ended.\\nAll of these four category questions include unan-\\nswerable questions that are not answerable solely\\nfrom the given document file. Models are expected\\nto generate answers in open-ended text generation\\nin any of these question types. Textual model in-\\nputs, or simply prompts, consist of the embedded\\ntexts or OCR results of documents as described\\nin Sec. 3.3 and the questions. We also include\\nthe answer-format guidelines such as “please an-\\nswer in Yes/No form”, “please answer the fact that\\nis referred to in the document”, “please answer\\nby numerical information from the document” and\\n“please write the answer in open-ended format” into\\nthe prompts depending on the four question cate-\\ngories. For unanswerable questions, we prepare a\\nspecial answer for all question categories: “本文中\\nに記載がありません(not mentioned in the text).”\\n4.2.\\nModels\\nWe conduct experiments with both text-input mod-\\nels and multimodal models of text and vision inputs.\\nFor model training, we use supervised finetuning.\\nThe best hyperparameters are searched with train\\nand validation sets, then the model performance is\\nevaluated with the best hyperparameters.\\n5The chance rate of yes/no questions including unan-\\nswerable is 61.57 when the model always marks “yes.”\\nModels with text input.\\nWe adapted up to 13\\nbillion (13B) model parameter scale Japanese\\nlarge language models for experiments.\\nWe\\nexperimented with the following representative\\nJapanese models that take only textual inputs:\\nrinna\\njapanese-gpt2-medium6,\\njapanese-gpt-\\n4B-8k7,\\nrinna\\njapanese-gpt-1B8,\\nCyberagent\\nOpenCALM-7B9,\\nMatsuo-Lab\\nweblab-10b10,\\nPFNet\\nPLaMo-13B11,\\nStability\\nAI\\nJapanese-\\nStableLM-Base-Alpha-7B12,\\nand\\nStability\\nAI\\nJapanese-StableLM-Instruct-Alpha-7B13.\\nWe\\nalso include multilingual large language model of\\nLlama-2-7B 14. We trained and evaluated models\\nwith 1024 token length for fair comparisons and\\ncomputational efficiency except rinna japanese-\\ngpt-4B-8k which is trained with 8192 tokens at\\nmost. For analyses with longer token lengths, we\\ntrain rinna japanese-gpt-4B-8k model with 2048,\\n4096, and 8192 tokens.\\nModels with multimodal input.\\nThe purpose\\nof JDocQA is to analyze documents with textual\\nand visual perceptions.\\nTo assess the impact\\nof using both images and text on the JDocQA\\ndataset, we applied multimodal models that take\\ninputs from both images and texts. We used Sta-\\nbility AI Japanese-StableLM-Instruct-Alpha-7B15, a\\nJapanese version of InstructBLIP (Dai et al., 2023;\\nLi et al., 2023a,c) for this purpose, as they are ap-\\nplicable to Japanese text and image inputs. We\\ntrained and evaluated this model with 512 token\\nlengths following its max capacity. We develop\\nthree different models for with three different visual\\ninputs as explained in Sec. 3.3. The first model\\ntakes visual inputs of a blank image that is always\\nthe same white image of the 800x600 pixel size as\\nablation study. The second model takes an image\\nof a whole document page that are related to the\\nquestion-answering in the annotation. These im-\\n6https://huggingface.co/rinna/\\njapanese-gpt2-medium\\n7https://huggingface.co/rinna/\\nbilingual-gpt-neox-4b-8k\\n8https://huggingface.co/rinna/\\njapanese-gpt-1b\\n9https://huggingface.co/cyberagent/\\nopen-calm-7b\\n10https://huggingface.co/matsuo-lab/\\nweblab-10b\\n11https://huggingface.co/pfnet/\\nplamo-13b\\n12https://huggingface.co/stabilityai/\\njapanese-stablelm-base-alpha-7b\\n13https://huggingface.co/stabilityai/\\njapanese-stablelm-instruct-alpha-7b\\n14https://huggingface.co/meta-llama/\\nLlama-2-7b-hf\\n15https://huggingface.co/stabilityai/\\njapanese-stablelm-instruct-alpha-7b\\nValidation set\\nTest set\\nModel\\nAvg.\\n(1) Y/N\\n(2) Fact.\\n(3) Num.\\n(4) Open.\\nAvg.\\n(1) Y/N\\n(2) Fact.\\n(3) Num.\\n(4) Open.\\nEvaluated with all instances.\\ngpt-3.5-turbo-16k\\n19.86\\n47.89\\n7.85\\n7.97\\n15.75\\n20.62\\n50.29\\n7.44\\n11.11\\n13.64\\ngpt-4\\n17.96\\n34.73\\n9.42\\n8.51\\n19.17\\n19.47\\n43.19\\n6.51\\n11.11\\n17.07\\nEvaluated without “unanswerable.”\\ngpt-3.5-turbo-16k\\n22.72\\n57.23\\n8.82\\n9.20\\n15.63\\n23.07\\n58.21\\n8.08\\n12.5\\n13.49\\ngpt-4\\n20.90\\n41.50\\n10.58\\n9.81\\n21.72\\n22.03\\n50.00\\n7.07\\n12.5\\n18.57\\nModels trained with all training instances and evaluated all instances.\\nrinna gpt2-medium-336M\\n21.33\\n63.15\\n7.32\\n4.78\\n17.51\\n19.41\\n62.13\\n4.65\\n8.18\\n15.99\\nrinna gpt-1B\\n23.79\\n58.42\\n10.99\\n6.38\\n22.27\\n20.46\\n59.76\\n5.58\\n8.77\\n18.13\\nrinna bi-4B-8k (8192 tok.)\\n26.35\\n55.26\\n14.65\\n13.29\\n24.93\\n23.02\\n62.13\\n8.83\\n11.11\\n20.57\\nOpenCALM-7B\\n21.65\\n47.36\\n14.65\\n5.31\\n20.81\\n18.33\\n43.78\\n11.62\\n9.94\\n16.03\\nweblab-10B\\n19.20\\n46.31\\n9.42\\n7.97\\n17.13\\n16.94\\n47.92\\n10.23\\n8.18\\n13.24\\nPLaMo-13B\\n25.79\\n55.26\\n15.18\\n15.42\\n22.92\\n20.33\\n53.84\\n10.69\\n7.01\\n18.21\\nStableLM Base-Al.-7B\\n32.92\\n67.89\\n21.98\\n18.61\\n29.62\\n29.71\\n70.41\\n15.81\\n22.22\\n25.51\\nStableLM Inst.-Al.-7B\\n33.80\\n67.36\\n20.94\\n20.21\\n31.39\\n29.56\\n72.78\\n16.27\\n21.05\\n24.75\\nLlama2-7B\\n30.29\\n60.00\\n20.41\\n15.42\\n28.59\\n27.01\\n61.53\\n17.20\\n18.71\\n23.29\\nModels trained with all training instances while evaluated without “unanswerable.”\\nrinna gpt2-medium-336M\\n22.22\\n69.81\\n4.70\\n3.68\\n18.71\\n19.61\\n65.75\\n4.54\\n6.57\\n16.31\\nrinna gpt-1B\\n22.84\\n62.26\\n7.05\\n5.52\\n21.14\\n20.18\\n64.38\\n4.04\\n8.55\\n17.40\\nrinna bi-4B-8k (8192 tok.)\\n24.09\\n52.83\\n12.94\\n10.42\\n23.10\\n21.17\\n62.32\\n6.56\\n7.89\\n19.11\\nOpenCALM-7B\\n20.75\\n50.31\\n12.35\\n4.90\\n19.20\\n17.53\\n46.57\\n9.59\\n8.55\\n15.10\\nweblab-10B\\n17.74\\n45.91\\n8.82\\n6.13\\n15.34\\n16.20\\n50.68\\n9.09\\n6.57\\n12.17\\nPLaMo-13B\\n22.66\\n54.08\\n10.58\\n9.81\\n20.76\\n18.35\\n50.68\\n8.58\\n5.26\\n16.87\\nStableLM Base-Al.-7B\\n31.55\\n70.44\\n18.82\\n15.95\\n28.24\\n28.33\\n71.91\\n12.62\\n21.05\\n24.32\\nStableLM Inst.-Al.-7B\\n31.74\\n69.81\\n17.05\\n15.95\\n29.55\\n28.66\\n76.02\\n12.62\\n19.07\\n24.40\\nLlama2-7B\\n28.78\\n62.26\\n17.05\\n14.72\\n26.44\\n25.70\\n65.06\\n12.62\\n18.42\\n21.87\\nModels trained without “unanswerable” while evaluated with all instances.\\nrinna gpt2-medium-336M\\n15.91\\n52.10\\n3.14\\n3.72\\n12.11\\n17.81\\n63.31\\n4.18\\n5.26\\n13.60\\nrinna gpt-1B\\n17.66\\n45.26\\n6.80\\n2.65\\n17.04\\n17.59\\n51.47\\n6.04\\n5.26\\n15.76\\nrinna bi-4B-8k (8192 tok.)\\n23.44\\n57.89\\n12.04\\n9.57\\n20.33\\n23.01\\n69.23\\n7.90\\n9.94\\n19.27\\nOpenCALM-7B\\n18.94\\n42.63\\n8.90\\n3.72\\n19.44\\n16.95\\n42.01\\n7.44\\n6.43\\n16.33\\nweblab-10B\\n20.43\\n50.00\\n9.42\\n6.91\\n18.70\\n17.96\\n52.07\\n6.51\\n8.18\\n15.34\\nPLaMo-13B\\n22.04\\n60.00\\n7.85\\n9.57\\n18.23\\n21.11\\n64.49\\n8.83\\n11.11\\n16.31\\nStableLM Base-Al.-7B\\n27.02\\n63.68\\n14.65\\n12.76\\n23.61\\n25.68\\n68.63\\n12.09\\n17.54\\n20.94\\nStableLM Inst.-Al.-7B\\n27.22\\n61.57\\n15.70\\n14.36\\n23.84\\n26.25\\n70.41\\n15.34\\n16.37\\n20.74\\nLlama2-7B\\n30.15\\n63.68\\n18.32\\n13.82\\n28.30\\n28.25\\n73.96\\n11.62\\n16.95\\n24.68\\nModels trained without “unanswerable” and evaluated without “unanswerable” instances.\\nrinna gpt2-medium-336M\\n18.75\\n62.26\\n3.52\\n4.29\\n14.33\\n20.12\\n73.28\\n4.54\\n5.92\\n15.41\\nrinna gpt-1B\\n21.15\\n54.08\\n7.64\\n3.06\\n21.17\\n20.02\\n59.58\\n6.56\\n5.92\\n18.20\\nrinna bi-4B-8k (8192 tok.)\\n27.63\\n69.18\\n13.52\\n11.04\\n24.26\\n25.95\\n80.13\\n8.58\\n11.18\\n21.79\\nOpenCALM-7B\\n20.55\\n50.94\\n10.00\\n4.29\\n19.67\\n17.97\\n48.63\\n8.08\\n7.23\\n16.32\\nweblab-10B\\n22.25\\n59.74\\n10.58\\n7.97\\n18.55\\n19.06\\n60.27\\n7.07\\n9.21\\n15.06\\nPLaMo-13B\\n26.48\\n71.69\\n8.82\\n11.04\\n22.74\\n24.29\\n74.65\\n9.59\\n12.50\\n19.34\\nStableLM Base-Al.-7B\\n32.30\\n76.10\\n16.47\\n14.72\\n29.15\\n29.21\\n79.45\\n13.13\\n19.73\\n24.14\\nStableLM Inst.-Al.-7B\\n32.41\\n73.58\\n17.64\\n16.56\\n29.16\\n29.75\\n81.50\\n16.66\\n18.42\\n23.69\\nLlama2-7B\\n33.11\\n76.10\\n20.58\\n15.95\\n28.84\\n30.57\\n85.61\\n12.62\\n19.07\\n25.47\\nTable 4: Results of all finetuned models and OpenAI GPT zeroshot. Avg. is weighted average of scores.\\nages are also scaled to 800-pixel width. The third\\nmodel take inputs of the images following the an-\\nnotated supporting facts. Following the annotated\\nbounding boxes, we crop the referenced regions\\nof the page images, combine the bounding boxes\\nand scale them for the model visual input. As some\\nquestions have several annotated supporting facts\\nto their answers, the combined image may contain\\nmore than one region of the annotated bounding\\nboxes. All of these multimodal models also take\\ntextual prompts that are similar to the text-input\\nmodels.\\nOpenAI GPT baselines.\\nWe also present Ope-\\nnAI GPT performances as baselines. Here we use\\ngpt-3.5-turbo-16k and gpt-4 models16. They take\\nsimilar prompts to those of text-input models. How-\\never, as they are the zero-shot models for our task,\\nwe observed they are quite sensitive to the prompts.\\nTo improve their performance, we manually tune\\nthe prompts for OpenAI GPT models. We avoid\\n16Latest model at Oct. 9 2023.\\nfinetuning OpenAI GPT models although finetuning\\nthem may greatly improve performances due to the\\nfollowing reasons. First, our purpose is to develop\\nlocal models that work on limited computational\\nresources, Second, the details of finetuning are\\nunavailable for these models, and finally due to the\\nAPI cost issues.\\n4.3.\\nEvaluation Methods\\nFor Yes/No, factoid, and numerical questions, we\\nused the exact match metric after trimming trivial\\ndifferences such as the presence or absence of\\npunctuation marks or Japanese suffix phrases such\\nas “です(is)” by simple rules. We have also exam-\\nined the variations of exact match, such as the ratio\\nof whether model prediction phrases are included\\nin the correct answer phrases or not. However, we\\nrealize that this metric performs quite similarly to\\nthe exact match evaluation and the difference is typ-\\nically less than 10 question-answer pairs in the vali-\\ndation set. For open-ended questions, answers are\\ntypically long, e.g., the average length of answers\\nis 65.97 characters in Japanese, and hence the ex-\\nact match does not work for evaluation. Therefore,\\nwe used BLEU score17 tokenized by MeCab18 for\\nautomatic evaluation of open-ended questions.\\n4.4.\\nExperimental Settings\\nOur dataset includes unanswerable questions in all\\nquestion categories. While it is expected that fine-\\ntuning models with unanswerable instances may\\nharness models to surpass illusion answers known\\nas hallucinations, detecting unanswerable ques-\\ntions is also notoriously difficult as of Rajpurkar\\net al. (2018). Therefore we prepare two types of\\nmodels for all base models experimented: models\\nfinetuned with all question answering pairs includ-\\ning unanswerable instances and models finetuned\\nwithout unanswerable instances. In the evaluation,\\nwe similarly prepare two separate validation and\\ntest sets: the standard validation and test sets that\\nconsists of all question-answering pairs, and the\\nsmaller validation and test sets where unanswer-\\nable questions are removed.\\n4.5.\\nResults\\nTable 4 presents the performance of text-input mod-\\nels for all question types on valid and test splits.\\nModels trained with all instances.\\nWe compare\\nthe first and third blocks in Table 4. They are the\\nresults of all JDocQA instances by zero-shot and\\nfinetuned models that are trained with all JDocQA\\ntraining instances including “unanswerable ques-\\ntions”. We realize that fine-tuned models outper-\\nformed gpt-3.5 and gpt-4 results especially when\\nthe model size is larger. Next, we compare the\\nsecond and fourth blocks in Table 4. They are the\\nsame models with previous block evaluations on\\nthe JDocQA without unanswerable questions. We\\nobserve a similar tendency to all instances evalua-\\ntions, suggesting that models finetuned with unan-\\nswerable instances perform similar performances\\nin both answerable and unanswerable questions.\\nAmong them, StableLM models perform best de-\\nspite their parameter size of 7B. The rinna bi-4B-8k\\nmodel also performs well despite its parameter size.\\nWe attribute this to its token length size of 8192\\nand will discuss this later. We notice that (1) yes/no\\nquestions are relatively easy although they do not\\nhave much effect on the averaged score (Avg.) that\\nmostly follows the most common question category\\nof (4) open-ended.\\n17https://github.com/mjpost/sacrebleu\\n18https://pypi.org/project/\\nmecab-python3/\\nTest set\\nModel\\nAvg.\\n(1) Y/N\\n(2) Fact. (3) Num. (4) Open.\\nTrained all and evaluated all.\\nInstBLIP (blank)\\n26.92\\n65.68\\n16.27\\n19.88\\n22.00\\nInstBLIP (img)\\n27.44\\n68.63\\n15.34\\n19.88\\n22.50\\nInstBLIP (bbox)\\n27.87\\n72.78\\n18.13\\n19.29\\n21.37\\nTrained all while evaluated w/o “unanswerable.”\\nInstBLIP (blank)\\n25.12\\n65.75\\n10.60\\n17.10\\n21.68\\nInstBLIP (img)\\n25.74\\n69.17\\n11.61\\n15.13\\n22.12\\nInstBLIP (bbox)\\n27.99\\n78.76\\n14.14\\n17.76\\n22.16\\nTrained w/o “unanswerable” while evaluated all.\\nInstBLIP (blank)\\n23.13\\n66.27\\n12.55\\n11.69\\n18.21\\nInstBLIP (img)\\n25.01\\n71.59\\n12.09\\n16.37\\n19.19\\nInstBLIP (bbox)\\n29.00\\n78.10\\n14.88\\n19.29\\n23.19\\nTrained w/o “unanswerable” and evaluated w/o “unanswerable.”\\nInstBLIP (blank)\\n26.45\\n76.71\\n13.63\\n13.15\\n21.26\\nInstBLIP (img)\\n28.52\\n82.87\\n13.13\\n18.42\\n22.25\\nInstBLIP (bbox)\\n27.79\\n80.13\\n11.61\\n16.44\\n22.71\\nTable 5: Results of multimodal input models. Avg.\\nis weighted average.\\nTest set\\nToken length\\nAvg.\\n(1) Y/N\\n(2) Fact. (3) Num. (4) Open.\\nTrained all and evaluated all.\\n2048 tokens\\n20.97\\n57.39\\n10.69\\n9.94\\n17.66\\n4096 tokens\\n21.96\\n56.21\\n9.30\\n13.45\\n19.38\\n8192 tokens\\n23.02\\n62.13\\n8.83\\n11.11\\n20.57\\nTrained w/o “unanswerable” and evaluated w/o “unanswerable.”\\n2048 tokens\\n24.57\\n72.60\\n10.10\\n9.21\\n21.18\\n4096 tokens\\n24.26\\n67.12\\n9.09\\n11.18\\n21.90\\n8192 tokens\\n25.95\\n80.13\\n8.58\\n11.18\\n21.79\\nTable 6:\\nResults of rinna bi-4B-8k models with\\ndifferent token length. Avg. is weighted average.\\nModels trained without unanswerable.\\nAs ex-\\nplained in Sec. 4.4, we also prepared models fine-\\ntuned without unanswerable questions in the train-\\ning instances. We present the evaluation results\\nincluding and excluding unanswerable questions in\\nthe fifth and sixth blocks in Table 4. It is quite inter-\\nesting when we compare the third and fifth block\\nresults in Table 4 as they share the same evalua-\\ntion set including unanswerable questions while the\\nmodels are finetuned with and without the unan-\\nswerable instances. Comparing the third and fifth\\nblock results, we notice almost all models finetuned\\nwith unanswerable questions perform better than\\ntheir answerable-only finetuned counterparts in the\\naveraged scores. Exceptions are the OpenCALM-\\n7B, weblab-10B and Llama2-7B models, which we\\nwill discuss in the next paragraph. We attribute\\nthis is due to the concept of hallucination, where\\nmodels generate answers that do not appear in\\ncontext texts. We will present an example of this in\\nthe qualitative analysis paragraph. Doping unan-\\nswerable instances may contribute to harnessing\\nhallucination in this experimental comparison.\\nOpenCALM-7B and weblab-10B do not predict\\nquestions as unanswerable so much.\\nIn the\\nthird block of Table 4, we notice interesting phe-\\nnomena: OpenCALM-7B and weblab-10B do not\\nperform well despite their parameter size. When\\nwe closely check these models’ outputs, we realize\\nTest set\\nModel\\nPamphlet\\nSlide\\nReport\\nWebsite\\nTrained all and evaluated all.\\nrinna gpt2-med-336M\\n18.62\\n16.32\\n14.57\\n3.09\\nrinna gpt-1B\\n16.66\\n15.10\\n15.08\\n4.72\\nrinna bi-4B-8k (8192)\\n21.81\\n16.73\\n18.41\\n3.53\\nOpenCALM-7B\\n15.19\\n15.10\\n13.04\\n2.56\\nweblab-10B\\n14.70\\n17.55\\n13.04\\n2.69\\nPLaMo-13B\\n21.56\\n13.06\\n15.85\\n2.90\\nBase-Al.-7B\\n26.96\\n20.40\\n24.04\\n4.64\\nInst-Al.-7B\\n27.69\\n23.26\\n23.01\\n5.71\\nInstBLIP-Al (blank)\\n25.49\\n22.04\\n21.48\\n3.79\\nInstBLIP-Al (img)\\n25.00\\n21.63\\n23.78\\n3.75\\nInstBLIP-Al (bbox)\\n25.00\\n22.04\\n22.50\\n4.20\\nTable 7:\\nDetailed file-type result. “Website” is\\nincluded only in test set as an out-of-domain set.\\nthese models, finetuned with all instances, predict\\n“本文中に記載がありません(not mentioned in the\\ntext)” much less than other models. OpenCALM-\\n7B and weblab-10B predicts 11.9% and 13.7% of\\nall instances are unanswerable while other mod-\\nels in the third block of Table 4 predict around or\\nmore than 20%. As they are trained with the same\\ndataset, we suspect this is due to their pretraining.\\nIt is also notable that predicting questions as unan-\\nswerable is a difficult task for models and often\\naffects the overall performance.\\nMultimodal model results.\\nWe present the re-\\nsults of models with multimodal inputs models\\nof StableLM-InstructBLIP-Alpha in Table 5. The\\nmodel performances are enhanced especially\\nwhen we use cropped images of referenced tables\\nor figures (bbox). We also notice that the model with\\nblack image inputs performs close to visual mod-\\nels to some extent, suggesting the effectiveness of\\ntextual inputs in our task. We also carefully note\\nthat the max token length of StableLM-InstructBLIP-\\nAlpha is 512, which can limit the textual understand-\\ning abilities of current multimodal models.\\nToken length dependency.\\nWe survey the to-\\nken length effects on the performance. For this\\npurpose, we finetuned three models of different to-\\nken lengths of rinna bi-4B-8k, e.g., 2048, 4096, and\\n8192 respectively for both all training instances and\\nwithout unanswerable conditions. We present the\\nperformances in Table 6. We notice the finetuning\\ntoken length surely affects the final result, although\\nwe also notice finetuning models with long token\\nlength are much more computationally costly than\\nthose of short token length models, which can be\\nthe reason for the good performance of the rinna\\nbi-4B-8k (8192 tokens) model in Table 4.\\nDetailed Analyses of document-types.\\nTable 7\\npresents the performance comparisons between\\neach file type.\\nFile types are classified into\\nJapanese pamphlets such as public relations book-\\nlets or magazines, slides such as presentation ma-\\nModel\\nHuman Evaluation ↑\\nTrained all and evaluated all.\\nPLaMo-13B\\n1.24\\nStableLM Instruct-Alpha-7B\\n1.49\\nStableLM InstructBLIP-Alpha (blank)\\n1.04\\nStableLM InstructBLIP-Alpha (img)\\n1.25\\nTable 8: Human evaluation on the sampled set.\\nterials, and report documents including figures and\\ntables. We also prepare the out-of-domain test set\\nof website scans where models still perform worse.\\nQualitative Analysis.\\nFigure 4 presents two ex-\\nample instances with the models’ generations. In\\nthe top example, we present the question, anno-\\ntated answer, and generation from three models\\ntrained with questions including unanswerable: Sta-\\nbleLM InstructBLIP-Alpha (img), StableLM Instruct-\\nAlpha-7B and StableLM Base-Alpha-7B respec-\\ntively. StableLM InstructBLIP-Alpha (img) can see\\nthe alignments of the table via visual inputs and gen-\\nerate reasonable descriptions for the car. StableLM\\nBase-Alpha also generates a similar answer while\\nit cannot attribute the running mileage of 123,334\\npresented in the table. At the bottom of Figure 4, we\\npresented the comparison of two finetuned models\\nthat originate from the same pretrained model and\\nare trained with and without unanswerable ques-\\ntions. The all instances model accurately predicts\\nthat there are no answers written in the text while\\nthe model without unanswerable questions falsely\\ngenerates the open-ended answers, causing the\\nphenomena known as hallucination.\\nHuman evaluation.\\nFinally, we performed the hu-\\nman evaluation on the sampled results for some\\nrepresentative models. We sampled 100 open-\\nended questions from the test set for this purpose.\\nWe choose two text-input models of PLaMo-13B\\nand StableLM Instruct-Alpha-7B. We also choose\\nmultimodal input models of StableLM InstructBLIP-\\nAlpha with back and image inputs. We ask an an-\\nnotator to attach scores from 0 to 2 for two criteria:\\nwhether the generated answers include the anno-\\ntated answer and whether the generated answers\\ndo not include wrong statements as questions’ an-\\nswers. The results are presented in Table 8. We no-\\ntice that StableLM Instruct-Alpha-7B outperforms\\nPLaMo-13B and image model outperforms its blank\\nimage counterpart.\\n5.\\nConclusion\\nWe introduced the JDocQA dataset concentrating\\nthe integration of both visual and linguistic cues in\\nquestion answering in Japanese. We incorporated\\nunanswerable questions from given documents,\\nwhich we confirmed is effective for harnessing the\\nhallucinated generation to some extent in our ex-\\nStableLM InstBLIP-Al (img):トヨタ・プロボックスです。平成14年124,344kmの⾛⾏距離です。\\nIt is TOYOTA PROBOX. Mileage in 2002 was 124,344km.\\nStableLM Inst.-Al.-7B:本⽂中に記載がありません\\nNot mentioned in the text\\nStableLM Base-Al.-7B:トヨタプロボックス平成14年124,344kmです。\\nTOYOTA PROBOX 2002 124,344km.\\nA:売却区分番号15J001は、トヨタの「プロボックス」です。初年度登録は平成14年で、⾛⾏距離(平成27\\n年6⽉30⽇時点)が124,344kmで、⾞検有効期限が平成27年10⽉13⽇の⾃動⾞です。/ ID \\n15J001 is TOYOTA PROBOX. Register date was 2002, total mileage was 124,344km, \\nand inspection expires 2015 Oct 13th.\\nQ:市税などの滞納により差し押さえをした⾃動⾞を公売しますが、売却区分番号15J001はどのような⾃動\\n⾞ですか︖/ The vehicle seized due to tax delinquent will be sold at a public auction. \\nPlease explain the feature of ID 15J001.\\nQ: 2016年のイランの実質GDPと消費者物価指数の推移に影響した全要因は何ですか/ What is the \\nmain factor affected to the Iranʼs actual GDP and consumer price index in 2016?\\nStableLM Inst.-Al.-7B (trained with all instances) :\\n本⽂中に記載がありません/ Not mentioned in the text\\nStableLM Inst.-Al.-7B (trained without unanswerable questions):\\n2016年はイランのGDPは18%もの成⻑を記録し、インフレ率も低下した。しかし、2018年の⽶国の制裁復\\n活が影響することで、IMEはGDP成⻑率が2018年に-1%、2019年には-4%に落ち込み、インフレ率は上\\n昇する。/ In 2016, Iran's GDP recorded an impressive growth of 18%, and the inflation \\nrate decreased. However, the re-imposition of U.S. sanctions in 2018 had a significant \\nimpact, causing Iran's GDP growth rate to plummet to -1% in 2018 and further to -\\n4% in 2019, while inflation rates rose.\\nA:本⽂中に記載がありません/Not mentioned in the text\\nModel comparison\\nTrain dataset comparison\\nFigure 4: Qualitative analysis of open-ended question answering.\\nperiments. Our detailed evaluations revealed the\\neffectiveness of our dataset in a wide range of ques-\\ntion categories from yes/no to open-ended and that\\nthe prediction of unanswerable questions can be\\na clue to improve model performances, illustrating\\nthe effectiveness of the JDocQA dataset in realistic\\napplications where multiple categories of questions\\nare feasible and some extent of questions do not\\nhave explicit written answers in the documents.\\n6.\\nAcknowledgments\\nThis work was supported by JSPS Grant-in-Aid\\nfor Young Scientists (#22K17983), JSPS Fostering\\nJoint International Research (A) (#22KK0184) and\\nby JST PRESTO (#JPMJPR20C2).\\nEthical Statement & Limitation\\nIn data collection, we gather document PDF files\\nand webpages from Japanese National Diet Library\\n(NDL)’s digital collection, web archive projects\\n(WARP), and websites of Japanese government\\nministries. Administrative PDFs, documents, pam-\\nphlets, or websites published by local governments\\nor universities are gathered through WARP. We\\ncarefully avoid private documents and choose con-\\nsiderably public documents published by public or\\nquasi-public sectors for the publicity of our dataset\\nusage. All of the documents and webpages are\\npublicly available online and we follow our institu-\\ntional rules to gather them. We follow our institu-\\ntional rules and also consult external advisors for\\ndata collection processes.\\nWe assume our datasets are useful for both re-\\nsearch and development of generative language\\nmodels and their applications for Japanese doc-\\nument question answering.\\nWe also consider\\nour dataset with unanswerable questions can con-\\ntribute to harnessing the hallucination problem of\\nlarge language models.\\nHowever, this doesn’t\\nmean that the fintuned models with unanswerable\\nquestions do not perform hallucinations at all.\\n7.\\nBibliographical References\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu,\\nMargaret Mitchell, Dhruv Batra, C. Lawrence Zit-\\nnick, and Devi Parikh. 2015. VQA: Visual Ques-\\ntion Answering. In International Conference on\\nComputer Vision (ICCV).\\nAnkan Bansal, Yuting Zhang, and Rama Chellappa.\\n2020. Visual question answering on image sets.\\nIn ECCV 2020.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sas-\\ntry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jef-\\nfrey Wu, Clemens Winter, Chris Hesse, Mark\\nChen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner,\\nSam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei. 2020. Language models are\\nfew-shot learners. In Advances in Neural Infor-\\nmation Processing Systems, volume 33, pages\\n1877–1901. Curran Associates, Inc.\\nXingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji,\\nDanyang Zhang, Ao Luo, Yuxuan Xiong, and\\nKai Yu. 2021.\\nWebSRC: A dataset for web-\\nbased structural reading comprehension. In Pro-\\nceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, pages\\n4173–4185, Online and Punta Cana, Dominican\\nRepublic. Association for Computational Linguis-\\ntics.\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\\nBoyang Li, Pascale Fung, and Steven Hoi. 2023.\\nInstructblip: Towards general-purpose vision-\\nlanguage models with instruction tuning.\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi,\\nGabriel Stanovsky, Sameer Singh, and Matt\\nGardner. 2019.\\nDROP: A reading compre-\\nhension benchmark requiring discrete reason-\\ning over paragraphs.\\nIn Proceedings of the\\n2019 Conference of the North American Chap-\\nter of the Association for Computational Linguis-\\ntics: Human Language Technologies, Volume\\n1 (Long and Short Papers), pages 2368–2378,\\nMinneapolis, Minnesota. Association for Compu-\\ntational Linguistics.\\nMohit Iyyer, Varun Manjunatha, Anupam Guha, Yo-\\ngarshi Vyas, Jordan L. Boyd-Graber, Hal Daumé,\\nand Larry S. Davis. 2016. The amazing mys-\\nteries of the gutter: Drawing inferences between\\npanels in comic book narratives. 2017 IEEE Con-\\nference on Computer Vision and Pattern Recog-\\nnition (CVPR), pages 6478–6487.\\nAniruddha\\nKembhavi,\\nMinjoon\\nSeo,\\nDustin\\nSchwenk, Jonghyun Choi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Are you smarter than\\na sixth grader? textbook question answering\\nfor multimodal machine comprehension. In Pro-\\nceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR).\\nKentaro Kurihara, Daisuke Kawahara, and Tomo-\\nhide Shibata. 2022. JGLUE: Japanese general\\nlanguage understanding evaluation. In Proceed-\\nings of the Thirteenth Language Resources and\\nEvaluation Conference, pages 2957–2966, Mar-\\nseille, France. European Language Resources\\nAssociation.\\nDongxu Li, Junnan Li, and Steven CH Hoi. 2023a.\\nBlip-diffusion: Pre-trained subject representation\\nfor controllable text-to-image generation and edit-\\ning. arXiv preprint arXiv:2305.14720.\\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang,\\nSilvio Savarese, and Steven CH Hoi. 2023b.\\nLavis: A one-stop library for language-vision in-\\ntelligence. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational\\nLinguistics (Volume 3: System Demonstrations),\\npages 31–41.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven\\nHoi. 2023c. BLIP-2: bootstrapping language-\\nimage pre-training with frozen image encoders\\nand large language models. In ICML.\\nMateusz Malinowski and Mario Fritz. 2014. A multi-\\nworld approach to question answering about real-\\nworld scenes based on uncertain input. In Ad-\\nvances in Neural Information Processing Sys-\\ntems, volume 27. Curran Associates, Inc.\\nMinesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthe-\\nnis Karatzas, Ernest Valveny, and C. V. Jawahar.\\n2022. Infographicvqa. In 2022 IEEE/CVF Winter\\nConference on Applications of Computer Vision\\n(WACV), pages 2582–2591.\\nMinesh Mathew, Dimosthenis Karatzas, and C. V.\\nJawahar. 2021a. Docvqa: A dataset for vqa on\\ndocument images. In Proceedings - 2021 IEEE\\nWinter Conference on Applications of Computer\\nVision, WACV 2021, pages 2199–2208.\\nMinesh Mathew, Dimosthenis Karatzas, and C. V.\\nJawahar. 2021b. DocVQA: A dataset for VQA\\non document images, Proceedings - 2021 IEEE\\nWinter Conference on Applications of Computer\\nVision, WACV 2021, pages 2199–2208. Institute\\nof Electrical and Electronics Engineers Inc. Fund-\\ning Information: We thank Amazon for supporting\\nthe annotation effort, and Dr. R. Manmatha for\\nmany useful discussions and inputs. This work is\\npartly supported by MeitY, Government of India,\\nthe project TIN2017-89779-P, an Amazon AWS\\nResearch Award and the CERCA Programme.\\nPublisher Copyright: © 2021 IEEE.\\nAnand Mishra, Shashank Shekhar, Ajeet Kumar\\nSingh, and Anirban Chakraborty. 2019. Ocr-vqa:\\nVisual question answering by reading text in im-\\nages. In ICDAR.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral,\\nand Hannaneh Hajishirzi. 2022. Cross-task gen-\\neralization via natural language crowdsourcing\\ninstructions. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pages 3470–\\n3487, Dublin, Ireland. Association for Computa-\\ntional Linguistics.\\nYusuke Miyao and Ai Kawazoe. 2013. University en-\\ntrance examinations as a benchmark resource for\\nNLP-based problem solving. In Proceedings of\\nthe Sixth International Joint Conference on Nat-\\nural Language Processing, pages 1357–1365,\\nNagoya, Japan. Asian Federation of Natural Lan-\\nguage Processing.\\nTakashi Miyazaki and Nobuyuki Shimizu. 2016.\\nCross-lingual image caption generation. In Pro-\\nceedings of the 54th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume\\n1: Long Papers), pages 1780–1790, Berlin, Ger-\\nmany. Association for Computational Linguistics.\\nOpenAI. 2023. GPT-4 technical report. Technical\\nreport.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke Miller, Maddie Simens, Amanda Askell, Pe-\\nter Welinder, Paul F Christiano, Jan Leike, and\\nRyan Lowe. 2022. Training language models\\nto follow instructions with human feedback. In\\nAdvances in Neural Information Processing Sys-\\ntems, volume 35, pages 27730–27744. Curran\\nAssociates, Inc.\\nRafał Powalski, Łukasz Borchmann, Dawid Ju-\\nrkiewicz, Tomasz Dwojak, Michał Pietruszka,\\nand Gabriela Pałka. 2021. Going full-tilt boo-\\ngie on document understanding with text-image-\\nlayout transformer. In Document Analysis and\\nRecognition–ICDAR 2021: 16th International\\nConference, Lausanne, Switzerland, Septem-\\nber 5–10, 2021, Proceedings, Part II 16, pages\\n732–747. Springer.\\nPranav Rajpurkar, Robin Jia, and Percy Liang.\\n2018. Know what you don’t know: Unanswer-\\nable questions for SQuAD. In Proceedings of the\\n56th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2: Short Papers),\\npages 784–789, Melbourne, Australia. Associa-\\ntion for Computational Linguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\\nrev, and Percy Liang. 2016. SQuAD: 100,000+\\nquestions for machine comprehension of text. In\\nProceedings of the 2016 Conference on Empir-\\nical Methods in Natural Language Processing,\\npages 2383–2392, Austin, Texas. Association\\nfor Computational Linguistics.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\\nM Saiful Bari, Canwen Xu, Urmish Thakker,\\nShanya Sharma Sharma, Eliza Szczechla, Tae-\\nwoon Kim, Gunjan Chhablani, Nihal Nayak, De-\\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Baw-\\nden, Thomas Wang, Trishala Neeraj, Jos Rozen,\\nAbheesht Sharma, Andrea Santilli, Thibault\\nFevry, Jason Alan Fries, Ryan Teehan, Teven Le\\nScao, Stella Biderman, Leo Gao, Thomas\\nWolf, and Alexander M Rush. 2022. Multitask\\nprompted training enables zero-shot task gener-\\nalization. In International Conference on Learn-\\ning Representations.\\nAmanpreet Singh, Vivek Natarajan, Meet Shah,\\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\\nand Marcus Rohrbach. 2019. Towards vqa mod-\\nels that can read.\\nIn The IEEE Conference\\non Computer Vision and Pattern Recognition\\n(CVPR).\\nBrandon Smock, Rohith Pesala, and Robin Abra-\\nham. 2022. Pubtables-1m: Towards compre-\\nhensive table extraction from unstructured docu-\\nments. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recogni-\\ntion (CVPR), pages 4634–4642.\\nNorio Takahashi, Tomohide Shibata, Daisuke\\nKawahara, and Sadao Kurohashi. 2019. Ma-\\nchine comprehension improves domain-specific\\nJapanese predicate-argument structure analy-\\nsis. In Proceedings of the 2nd Workshop on Ma-\\nchine Reading for Question Answering, pages\\n98–104, Hong Kong, China. Association for Com-\\nputational Linguistics.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021.\\nMultimodalqa: complex question answering over\\ntext, tables and images.\\nIn 9th International\\nConference on Learning Representations, ICLR\\n2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nRyota Tanaka, Kyosuke Nishida, Kosuke Nishida,\\nTaku Hasegawa, Itsumi Saito, and Kuniko Saito.\\n2023. Slidevqa: A dataset for document visual\\nquestion answering on multiple images. In AAAI.\\nRyota Tanaka, Kyosuke Nishida, and Sen Yoshida.\\n2021. Visualmrc: Machine reading comprehen-\\nsion on document images. In AAAI.\\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\\nYang Liu, Chenguang Zhu, Michael Zeng, Chao-\\nYue Zhang, and Mohit Bansal. 2023. Unifying vi-\\nsion, text, and layout for universal document pro-\\ncessing. 2023 IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR),\\npages 19254–19264.\\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefel-\\nhagen, Antonio Torralba, Raquel Urtasun, and\\nSanja Fidler. 2016. MovieQA: Understanding\\nStories in Movies through Question-Answering.\\nIn IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR).\\nRubèn Pérez Tito, Dimosthenis Karatzas, and\\nErnest Valveny. 2021. Document collection vi-\\nsual question answering. In IEEE International\\nConference on Document Analysis and Recog-\\nnition.\\nRubèn Pérez Tito, Dimosthenis Karatzas, and\\nErnest Valveny. 2022. Hierarchical multimodal\\ntransformers for multi-page docvqa.\\nArXiv,\\nabs/2212.05935.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2022. Finetuned\\nlanguage models are zero-shot learners. In In-\\nternational Conference on Learning Representa-\\ntions.\\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio,\\nCha Zhang, Wanxiang Che, Min Zhang, and Li-\\ndong Zhou. 2021. LayoutLMv2: Multi-modal pre-\\ntraining for visually-rich document understand-\\ning. In Proceedings of the 59th Annual Meeting\\nof the Association for Computational Linguistics\\nand the 11th International Joint Conference on\\nNatural Language Processing (Volume 1: Long\\nPapers), pages 2579–2591, Online. Association\\nfor Computational Linguistics.\\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang,\\nFuru Wei, and Ming Zhou. 2020. Layoutlm: Pre-\\ntraining of text and layout for document image\\nunderstanding. In Proceedings of the 26th ACM\\nSIGKDD International Conference on Knowl-\\nedge Discovery & Data Mining, KDD ’20, page\\n1192–1200, New York, NY, USA. Association for\\nComputing Machinery.\\nSemih Yagcioglu, Aykut Erdem, Erkut Erdem, and\\nNazli Ikizler-Cinbis. 2018. RecipeQA: A chal-\\nlenge dataset for multimodal comprehension of\\ncooking recipes. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 1358–1368, Brussels,\\nBelgium. Association for Computational Linguis-\\ntics.\\nHitomi Yanaka and Koji Mineshima. 2022. Composi-\\ntional evaluation on Japanese textual entailment\\nand similarity. Transactions of the Association\\nfor Computational Linguistics, 10:1266–1284.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua\\nBengio, William Cohen, Ruslan Salakhutdinov,\\nand Christopher D. Manning. 2018. HotpotQA: A\\ndataset for diverse, explainable multi-hop ques-\\ntion answering. In Proceedings of the 2018 Con-\\nference on Empirical Methods in Natural Lan-\\nguage Processing, pages 2369–2380, Brussels,\\nBelgium. Association for Computational Linguis-\\ntics.\\n\"),\n",
       " Document(metadata={'Published': '2021-01-27', 'Title': 'Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?', 'Authors': 'Yixuan Tang, Hwee Tou Ng, Anthony K. H. Tung', 'Summary': 'Multi-hop question answering (QA) requires a model to retrieve and integrate\\ninformation from different parts of a long text to answer a question. Humans\\nanswer this kind of complex questions via a divide-and-conquer approach. In\\nthis paper, we investigate whether top-performing models for multi-hop\\nquestions understand the underlying sub-questions like humans. We adopt a\\nneural decomposition model to generate sub-questions for a multi-hop complex\\nquestion, followed by extracting the corresponding sub-answers. We show that\\nmultiple state-of-the-art multi-hop QA models fail to correctly answer a large\\nportion of sub-questions, although their corresponding multi-hop questions are\\ncorrectly answered. This indicates that these models manage to answer the\\nmulti-hop questions using some partial clues, instead of truly understanding\\nthe reasoning paths. We also propose a new model which significantly improves\\nthe performance on answering the sub-questions. Our work takes a step forward\\ntowards building a more explainable multi-hop QA system.'}, page_content='Do Multi-Hop Question Answering Systems Know\\nHow to Answer the Single-Hop Sub-Questions?\\nYixuan Tang\\nHwee Tou Ng\\nAnthony K.H. Tung\\nDepartment of Computer Science\\nNational University of Singapore\\n{yixuan, nght, atung}@comp.nus.edu.sg\\nAbstract\\nMulti-hop question answering (QA) requires\\na model to retrieve and integrate information\\nfrom multiple passages to answer a question.\\nRapid progress has been made on multi-hop\\nQA systems with regard to standard evalua-\\ntion metrics, including EM and F1. However,\\nby simply evaluating the correctness of the an-\\nswers, it is unclear to what extent these sys-\\ntems have learned the ability to perform multi-\\nhop reasoning. In this paper, we propose an ad-\\nditional sub-question evaluation for the multi-\\nhop QA dataset HotpotQA, in order to shed\\nsome light on explaining the reasoning process\\nof QA systems in answering complex ques-\\ntions. We adopt a neural decomposition model\\nto generate sub-questions for a multi-hop ques-\\ntion, followed by extracting the corresponding\\nsub-answers. Contrary to our expectation, mul-\\ntiple state-of-the-art multi-hop QA models fail\\nto answer a large portion of sub-questions, al-\\nthough the corresponding multi-hop questions\\nare correctly answered. Our work takes a step\\nforward towards building a more explainable\\nmulti-hop QA system.\\n1\\nIntroduction\\nRapid progress has been made in the ﬁeld of\\nquestion answering (QA), thanks to the release of\\nmany large-scale, high-quality QA datasets. Early\\ndatasets (Hermann et al., 2015; Rajpurkar et al.,\\n2016, 2018; Trischler et al., 2017; Joshi et al., 2017)\\nmainly consist of single-hop questions, where an\\nanswer with supporting justiﬁcation can be found\\nwithin a short segment of text. These benchmarks\\nfocus on evaluating QA models’ ability to perform\\nlocal pattern matching between a passage and a\\nquestion. Existing models (Lan et al., 2020; Zhang\\net al., 2020) have achieved super-human perfor-\\nmance. Recently, multi-hop QA datasets (Khashabi\\net al., 2018; Welbl et al., 2018; Yang et al., 2018)\\nhave gained increasing attention. They require\\nmodels to retrieve multiple pieces of supporting\\nevidence from different documents and to reason\\nover the evidence collected to answer a question.\\nThe standard evaluation metrics of QA datasets in-\\nclude exact match (EM) and F1 scores averaged\\nover the test set. HotpotQA (Yang et al., 2018) also\\nprovides sentence-level supporting facts required\\nfor reasoning. However, providing supporting sen-\\ntences is not sufﬁcient for us to interpret the choice\\nof an answer for end-to-end complex QA systems.\\nIt is unclear whether the systems have performed\\nthe desired multi-hop reasoning to reach the correct\\nanswer.\\nIn this work, we propose an additional evalua-\\ntion scheme to test multi-hop QA systems’ perfor-\\nmance on answering the single-hop sub-questions\\nof a multi-hop question. When designing a multi-\\nhop question, we expect it to require QA models\\nto retrieve a chain of sentences as evidence and\\nthen reasoning over them to answer the question.\\nEvaluating QA models on sub-questions helps us\\nto understand their behavior on each hop of the\\nreasoning process. In addition, it evaluates whether\\nmulti-hop QA models can generalize well on sim-\\npler questions. Figure 1 presents an illustrating\\nexample. A successful complex QA model should\\nbe able to answer the two sub-questions “Which\\nmovie stars Arnold Schwarzenegger as a former\\nNew York Police detective” and “What year did\\nGuns N Roses perform a promo for End of Days”\\nif it understands the underlying reasoning process\\nfor the original multi-hop question.\\nWe focus on the HotpotQA (Yang et al., 2018)\\ndataset under the distractor setting, in which multi-\\nhop questions are asked over several Wikipedia\\nparagraphs. We create the evaluation dataset by\\ngenerating the sub-questions and then extracting\\ntheir answers automatically. The candidate sub-\\nquestions and intermediate answers are then man-\\nually veriﬁed, which results in 1,000 sub-question\\narXiv:2002.09919v2  [cs.CL]  27 Jan 2021\\nFigure 1: An illustrating example from the HotpotQA dataset in the distractor setting, with our construction pro-\\ncedure to generate an evaluation example. We only show one out of eight distracting paragraphs provided in the\\ncontext due to paper length constraint.\\nevaluation examples. It is surprising to ﬁnd that all\\nthree top-performing models which we experiment\\nwith fail to answer a large portion of sub-questions\\n(49.8% to 60.4%), although their corresponding\\nmulti-hop questions are correctly answered.\\nPrevious work has investigated the necessity of\\nmulti-hop reasoning on HotpotQA dataset. Jiang\\nand Bansal (2019) construct distracting paragraphs\\nadversarially to demonstrate that models learn to\\nexploit reasoning shortcuts to locate the answer\\nrather than performing multi-hop reasoning. Chen\\nand Durrett (2019) show that a sentence-factored\\nmodel can solve a large number of questions in Hot-\\npotQA, suggesting multi-hop reasoning is not re-\\nally needed. Min et al. (2019a) also achieve similar\\nresult using a single-hop BERT-based model. Our\\nsub-question evaluation is complementary to these\\napproaches. While existing work shows the lack\\nof multi-hop reasoning by limiting or adding text\\ninput to QA models, we provide sub-questions and\\nintermediate answers explicitly to interpret model\\nbehavior on each hop of the reasoning process. It\\ncan be used as a complementary metric to ensure\\nthat models which can correctly answer both in-\\ntermediate sub-questions and the ﬁnal multi-hop\\nquestion actually go through the reasoning steps\\nas desired. Our work takes a step forward towards\\nbuilding a more explainable multi-hop QA system.\\n2\\nConstruction of Evaluation Examples\\nIn this section, we introduce our semi-automatic\\napproach to generate two sub-questions and their\\ncorresponding answers for multi-hop questions\\nfrom the HotpotQA dataset. As shown in Figure\\n1, the evaluation examples are generated in three\\nsteps. First, we decompose each source question\\ninto several sub-strings by predicting the break-\\ning points and post-process them to generate two\\nsub-questions.\\nThen, the answers for the sub-\\nquestions are extracted from the paragraphs using\\nsome heuristics. Lastly, the candidate evaluation\\nexamples generated are sent for human veriﬁcation.\\nWe ﬁrst introduce the HotpotQA dataset and then\\nelaborate on each step of the construction pipeline.\\n2.1\\nHotpotQA\\nHotpotQA contains 113K crowd-sourced multi-\\nhop QA pairs on Wikipedia articles. We focus\\non bridge-type questions that actually require mul-\\ntiple steps of reasoning under the distractor set-\\nting. During the construction of such an example\\nin HotpotQA, two related paragraphs pgold1, pgold2\\nfrom different Wikipedia articles titled tgold1, tgold2\\nare presented to crowd-workers. The two para-\\ngraphs are related since the text content in one\\nparagraph contains the title entity of the other para-\\ngraph. This shared title entity is referred to as\\nthe bridge entity.\\nUsing Figure 1 as an exam-\\nple, the second paragraph about Oh My God con-\\ntains the title entity of the ﬁrst paragraph, End\\nof Days (underlined). Thus, End of Days is re-\\nferred as the bridge entity. The crowd-workers\\nare encouraged to ask a multi-hop question using\\nboth paragraphs and to annotate the supporting sen-\\ntences which help to determine the answer. Then,\\neight other related distracting paragraphs are re-\\ntrieved from Wikipedia and mixed with the two\\ngold paragraphs to serve as the context for the ques-\\ntion. Given an example E = {C, q, a} from Hot-\\npotQA, we aim to generate an evaluation example\\nE′\\n=\\n{C, q, a, sub q1, sub a1, sub q2, sub a2},\\nwhere sub q1 and sub q2 are the two sub-questions,\\nand sub a1 and sub a2 are their corresponding an-\\nswers.\\n2.2\\nSub-Question Generation\\nGiven a multi-hop question, the ﬁrst step is to de-\\ncompose it into sub-questions. We adopt the model\\nintroduced in DecompRC (Min et al., 2019b) to\\ngenerate the sub-questions using a copying and\\nediting mechanism. The multi-hop question is ﬁrst\\nconverted into BERT word embeddings (Devlin\\net al., 2019), and then sent to a fully connected\\nneural network to predict the splitting points. It\\nis trained on 400 annotated examples. The sepa-\\nrated text spans are post-processed to form the two\\nsub-questions, following a set of handcrafted rules.\\n2.3\\nIntermediate Answer Extraction\\nOne particular characteristic of bridge-type ques-\\ntions from HotpotQA is that the two gold para-\\ngraphs are linked by a bridge entity. Since the\\ncrowd-workers are required to form a multi-hop\\nquestion which makes use of information from\\nboth paragraphs, there is a high probability that the\\nbridge entity is the answer to the ﬁrst sub-question.\\nFor the example shown in Figure 1, End of Days\\nin gold paragraph 2 is the bridge entity. It is also\\nthe intermediate answer for the multi-hop question,\\ni.e., the answer for the ﬁrst sub-question.\\nThree different situations are considered in or-\\nder to extract the bridge entity. First, if the title\\nentity EA of paragraph A occurs in the other para-\\ngraph B, while the title entity EB of B does not\\nCase Gold Answer\\nPredicted Answer\\n1\\nfrom 1986 to 2013\\n1986 to 2013\\n2\\nCity of Angles (ﬁlm)\\nCity of Angles\\n3\\nMondelez\\nInternational, Inc.\\nthe company\\nMondelez\\nInternational\\nTable 1: Examples of partially matched answer string\\npairs.\\nModel\\nq\\nqsub1\\nqsub2\\nEM\\nF1\\nEM\\nF1\\nEM\\nF1\\nDFGN\\n58.1 71.96\\n54.6 68.54\\n49.3\\n60.83\\nDecompRC 63.1 77.61\\n61\\n75.21\\n56.8\\n70.77\\nCogQA\\n53.2 67.82\\n58.6 69.65\\n54\\n68.49\\nTable 2: EM and F1 scores of models on 1,000 human-\\nveriﬁed sub-question evaluation examples.\\noccur in A, then EA is recognized as the bridge en-\\ntity. Second, if neither EA nor EB is contained in\\nthe other paragraph, then the title entity with more\\noverlapping text with the other paragraph is chosen\\nas the bridge entity (since sometimes the alias of\\nthe Wikipedia title is used in the paragraph). Lastly,\\nif both EA and EB appear in the other paragraph,\\nthen the title entity which does not appear in both\\nthe question and the answer is chosen as the bridge\\nentity, since an entity mentioned in the multi-hop\\nquestion or included in the ﬁnal answer is unlikely\\nto be the bridge entity. The bridge entity is set to be\\nunidentiﬁed if none or both of the title entities sat-\\nisfy at least one of the requirements. As illustrated\\nin Figure 1, once the bridge entity is retrieved, the\\nblank in the second sub-question will be updated.\\nThe answer to the second sub-question should be\\nthe same as the original multi-hop question.\\n2.4\\nHuman Veriﬁcation\\nSub-question generation and intermediate answer\\nextraction help to efﬁciently generate candidate\\nsub-questions and their answers. To ensure the\\nquality of the evaluation dataset, the examples gen-\\nerated are manually veriﬁed. For each example, we\\npresent to an annotator the original multi-hop ques-\\ntion, the answer, two sub-questions generated and\\ntheir answers, and two gold paragraphs. Questions\\nthat actually do not require multi-hop reasoning or\\nwith the wrong answer (due to wrong annotation by\\nthe HotpotQA crowd workers) are ﬁrst ﬁltered out.\\nThen, the annotator is required to review whether\\nsub q1 and sub q2 are two syntactically and se-\\nmantically correct sub-questions of q and whether\\nsub a1 and sub a2 are valid and to correct them if\\nnot. In total, a sample of 1,000 examples generated\\nfor the HotpotQA development set are manually\\nveriﬁed for use in our evaluation1.\\n3\\nExperiments and Results\\nIn order to interpret the behavior of existing mod-\\nels on each hop of the reasoning process required\\nfor multi-hop questions and to determine their\\nability to answer simple questions, we perform\\nsub-question evaluation on three published top-\\nperforming QA models with publicly available\\nopen-source code: DFGN (Qiu et al., 2019), De-\\ncompRC (Min et al., 2019b), and CogQA (Ding\\net al., 2019). For all experiments, we measure EM\\nand F1 scores for q, sub q1, and sub q2 on 1,000\\nhuman-veriﬁed examples. To measure the correct-\\nness of a predicted answer, we ﬁrst use exact string\\nmatch as the only metric. However, during error\\nanalysis, we ﬁnd that many predicted answers that\\npartially match the gold answers should also be\\nregarded as correct. Some representative exam-\\nples are shown in Table 1. Although these pre-\\ndicted answers have zero EM scores, they are se-\\nmantically equivalent to the correct answers given.\\nTherefore, we deﬁne a more ﬂexible metric named\\npartial match (PM) as an additional evaluation of\\ncorrectness. Given a gold answer text span ag and\\na predicted answer text ap, they partially match\\nif either one of the following two requirements is\\nsatisﬁed:\\nf1 > 0.8\\nf1 > 0.6 ∧{(ag contains ap) ∨(ap contains ag)}\\n(1)\\nTable 2 shows the performance of the three\\nmodels on multi-hop questions and their single-\\nhop sub-questions. Compared to multi-hop ques-\\ntions, the performance of DFGN and Decom-\\npRC drops on simpler sub-questions, especially\\non the second sub-questions (11.13 F1 reduction\\nfor DFGN and 6.84 F1 reduction for DecompRC).\\nCogQA achieves slightly better performance on\\nsub-questions, which shows that it is also able\\nto answer single-hop questions. The EM and F1\\nscores are averaged over all examples. In order\\nto understand whether models are able to answer\\n1The veriﬁed dataset is available at https://github.com/\\nyxxytang/subqa\\nq\\nqsub1 qsub2 DFGN DecompRC CogQA\\nc\\nc\\nc\\n23.0\\n31.3\\n26.7\\nc\\nc\\nw\\n9.7\\n7.2\\n5.8\\nc\\nw\\nc\\n17.9\\n19.1\\n17.8\\nc\\nw\\nw\\n7.5\\n5.5\\n2.9\\nw\\nc\\nc\\n4.9\\n3.0\\n3.6\\nw\\nc\\nw\\n17.0\\n18.6\\n22.5\\nw\\nw\\nc\\n3.5\\n3.4\\n5.9\\nw\\nw\\nw\\n16.5\\n11.9\\n14.8\\nTable 3: Categorical EM statistics (%) of sub-question\\nevaluation for the three models. Under the ﬁrst three\\ncolumns, c stands for correct and w stands for wrong.\\nFor example, the second row shows the percentage of\\nquestions where models correctly answer both multi-\\nhop question and the ﬁrst sub-question but wrongly an-\\nswer the second sub-question.\\nq\\nqsub1 qsub2 DFGN DecompRC CogQA\\nc\\nc\\nc\\n36.3\\n47.4\\n40.9\\nc\\nc\\nw\\n11.9\\n8.5\\n6.1\\nc\\nw\\nc\\n16.4\\n17.2\\n16.5\\nc\\nw\\nw\\n6.5\\n3.9\\n3.4\\nw\\nc\\nc\\n4.2\\n4.0\\n4.5\\nw\\nc\\nw\\n12.1\\n11.1\\n15.2\\nw\\nw\\nc\\n3.1\\n1.9\\n5.6\\nw\\nw\\nw\\n9.5\\n6.0\\n7.8\\nTable 4: Categorical PM statistics (%) of sub-question\\nevaluation for the three models.\\nthe sub-questions of correctly answered multi-hop\\nquestions, we collect the correctness statistics with\\nregard to each individual example. Table 3 and\\nTable 4 present the results. The ﬁrst four rows\\nshow the percentage of examples whose multi-hop\\nquestion can be correctly answered. Among these\\nexamples, we notice that there is a high probability\\nthat the models fail to answer at least one of the\\nsub-questions, as shown in rows 2 to 4. We refer\\nto these examples as model failure cases. The per-\\ncentage of model failure cases over all correctly\\nanswered multi-hop questions is deﬁned as model\\nfailure rate. As shown in Figure 2, all three models\\nevaluated have a high model failure rate, indicating\\nthat the models learn to answer the complex ques-\\ntions without exploring the multiple steps of rea-\\nsoning process as desired. The same phenomenon\\nappears when evaluated using exact match and the\\nless strict partial match scores.\\nAfter analyzing the model failure cases, we ob-\\nserve a common phenomenon that there is a high\\nFigure 2: Model failure rates under EM and PM.\\nsimilarity between the words in the second sub-\\nquestion and the words near the answer in the con-\\ntext. The model has learned to answer multi-hop\\nquestion by local pattern matching, instead of going\\nthrough the multiple reasoning steps. For the ex-\\nample presented in Figure 1, the model may locate\\nthe answer “1999” for the multi-hop question by\\nmatching the surrounding words “ Guns N Roses”\\nin the second sub-question. Despite answering the\\nmulti-hop question correctly, the model fails to\\nidentify the answer of the ﬁrst sub-question which\\nit is expected to retrieve as a multi-hop QA system.\\n4\\nConclusion\\nWe propose a new way to interpret whether multi-\\nhop QA systems explore the multiple steps of rea-\\nsoning over the evidence as desired by asking sub-\\nquestions.\\nAn automatic approach is designed\\nto generate sub-questions for a multi-hop ques-\\ntion.\\nOn a human-veriﬁed test set, our experi-\\nments demonstrate that top-performing multi-hop\\nQA models fail to answer a large portion of sub-\\nquestions whose parent multi-hop questions can\\nbe correctly answered. We believe that progress\\non building complex QA systems that truly under-\\nstand multi-hop reasoning is only possible if the\\nevaluation metrics reward this kind of behavior.\\nAs an initial step towards a more explainable QA\\nsystem, we hope our work would motivate the con-\\nstruction of multi-hop QA datasets with explicit\\nreasoning paths annotated and the development of\\nbetter multi-hop QA models.\\nReferences\\nJifan Chen and Greg Durrett. 2019.\\nUnderstanding\\ndataset design choices for multi-hop reasoning. In\\nNAACL-HLT, pages 4026–4032.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019.\\nBERT: pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL-HLT, pages 4171–4186.\\nMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang,\\nand Jie Tang. 2019. Cognitive graph for multi-hop\\nreading comprehension at scale.\\nIn ACL, pages\\n2694–2703.\\nKarl Moritz Hermann, Tom´as Kocisk´y, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In NIPS, pages 1693–1701.\\nYichen Jiang and Mohit Bansal. 2019. Avoiding rea-\\nsoning shortcuts: Adversarial evaluation, training,\\nand model development for multi-hop QA. In ACL,\\npages 2726–2736.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017.\\nTriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In ACL, pages 1601–1611.\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\\nShyam Upadhyay, and Dan Roth. 2018.\\nLooking\\nbeyond the surface: A challenge set for reading\\ncomprehension over multiple sentences. In NAACL-\\nHLT, pages 252–262.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\\n2020.\\nALBERT: A lite BERT for self-supervised\\nlearning of language representations. In ICLR.\\nSewon Min, Eric Wallace, Sameer Singh, Matt Gard-\\nner, Hannaneh Hajishirzi, and Luke Zettlemoyer.\\n2019a. Compositional questions do not necessitate\\nmulti-hop reasoning. In ACL, pages 4249–4257.\\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Multi-hop reading compre-\\nhension through question decomposition and rescor-\\ning. In ACL, pages 6097–6109.\\nLin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li,\\nWeinan Zhang, and Yong Yu. 2019. Dynamically\\nfused graph network for multi-hop reasoning.\\nIn\\nACL, pages 6140–6150.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for SQuAD. In ACL, pages 784–789.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100, 000+ questions\\nfor machine comprehension of text.\\nIn EMNLP,\\npages 2383–2392.\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\\nris, Alessandro Sordoni, Philip Bachman, and Ka-\\nheer Suleman. 2017.\\nNewsQA: A machine com-\\nprehension dataset. In Rep4NLP@ACL, pages 191–\\n200.\\nJohannes Welbl, Pontus Stenetorp, and Sebastian\\nRiedel. 2018. Constructing datasets for multi-hop\\nreading comprehension across documents. In TACL,\\npages 287–302.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\\ngio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. 2018.\\nHotpotQA: A\\ndataset for diverse, explainable multi-hop question\\nanswering. In EMNLP, pages 2369–2380.\\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\\nDuan, Hai Zhao, and Rui Wang. 2020.\\nSG-Net:\\nsyntax-guided machine reading comprehension. In\\nAAAI, pages 9636–9643.\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb5998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
